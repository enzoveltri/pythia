{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89237389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import get_close_matches, SequenceMatcher\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from random import choice, randrange\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "!pip install wikipedia\n",
    "import wikipedia\n",
    "!pip install pyinterval\n",
    "import os\n",
    "import tarfile\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c680205",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data prep\n",
    "ps = PorterStemmer()\n",
    "stemmed_words_list = [ps.stem(w) for w in words.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367225ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils funct\n",
    "def saveCache(fileName, cache):\n",
    "    tempDict = {}\n",
    "    for key, value in cache.items():\n",
    "        tempDict[key] = list(value)\n",
    "    with open(fileName, 'w') as fp:\n",
    "        json.dump(tempDict, fp)\n",
    "        \n",
    "def loadCache(fileName):\n",
    "    data = None\n",
    "    with open(fileName, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    cache = {}\n",
    "    for key, value in data.items():\n",
    "        cache[key] = set(value)\n",
    "    return cache\n",
    "\n",
    "def findFromCached(cachedAlias, cachedProvenance, columns):\n",
    "    aliasReturn = {}\n",
    "    provenanceReturn = {}\n",
    "    columnsToSearch = set()\n",
    "    for column in columns:\n",
    "        if column in cachedAlias:\n",
    "            aliasReturn[column] = cachedAlias[column]\n",
    "        else:\n",
    "            columnsToSearch.add(column)\n",
    "        if column in cachedProvenance:\n",
    "            provenanceReturn[column] = cachedProvenance[column]\n",
    "        else:\n",
    "            columnsToSearch.add(column)\n",
    "     \n",
    "    return aliasReturn, provenanceReturn, columnsToSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## caches\n",
    "cacheSynonym = {}\n",
    "cacheRelatedTo = {}\n",
    "cacheIsA = {}\n",
    "cacheDerivedFrom = {}\n",
    "cacheWikipedia = {}\n",
    "cachedAlias = {}\n",
    "cachedProvenance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load caches\n",
    "cacheSynonym = loadCache('./cacheSynonym-small.json')\n",
    "cacheRelatedTo = loadCache('./cacheRelatedTo-small.json')\n",
    "cacheIsA = loadCache('./cacheIsA-small.json')\n",
    "cacheDerivedFrom = loadCache('./cacheDerivedFrom-small.json')\n",
    "cacheWikipedia = loadCache('./cacheWikipedia-small.json')\n",
    "cachedAlias = loadCache('./cacheAlias.json')\n",
    "#cachedProvenance = loadCache('./cacheProvenance.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f46460",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"cacheProvenance.pkl\", \"rb\")\n",
    "cachedProvenance = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a84094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLCS(string1, string2):\n",
    "    match = SequenceMatcher(None, string1, string2).find_longest_match(0, len(string1), 0, len(string2))\n",
    "    return string1[match.a: match.a + match.size]\n",
    "    #return string2[match.b: match.b + match.size]\n",
    "\n",
    "## CONCEPTNET.IO ##\n",
    "def getSynonym(word, limit=10, dropWord=True, useCache=True):\n",
    "    processedText = word.lower().replace(' ', '_')\n",
    "    synonyms = set()\n",
    "    if (len(processedText) <= 1):\n",
    "        return synonyms, True\n",
    "    if useCache and word in cacheSynonym:\n",
    "        return cacheSynonym[word], True\n",
    "    baseUrl = \"https://api.conceptnet.io/query?node=/c/en/$WORD$&other=/c/en&limit=$LIMIT$&rel=/r/Synonym\"\n",
    "    processedURL = baseUrl.replace(\"$WORD$\", processedText)\n",
    "    processedURL = processedURL.replace(\"$LIMIT$\", str(limit))\n",
    "    #print(\"Processed URL: \", processedURL)\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = requests.get(processedURL).json()\n",
    "    except Exception:\n",
    "        return synonyms, False\n",
    "    #obj = requests.get(processedURL).json()\n",
    "    if 'edges' not in obj:\n",
    "        return synonyms, False\n",
    "    resultsLen = len(obj['edges'])\n",
    "    edges = obj['edges']\n",
    "    #print(\"Results:\", resultsLen)\n",
    "    \n",
    "    for i in range(0, resultsLen):\n",
    "        edge = edges[i]\n",
    "        label = edge['end']['label']\n",
    "        synonyms.add(label.lower())\n",
    "        label = edge['start']['label']\n",
    "        synonyms.add(label.lower())\n",
    "    if dropWord:\n",
    "        synonyms.discard(word.lower())\n",
    "    if useCache:\n",
    "        cacheSynonym[word] = synonyms\n",
    "    return synonyms, False\n",
    "\n",
    "def getRelatedTo(word, limit=10, dropWord=True, useCache=True):\n",
    "    processedText = word.lower().replace(' ', '_')\n",
    "    relatedTo = set()\n",
    "    if (len(processedText) <= 1):\n",
    "        return relatedTo, True\n",
    "    if useCache and word in cacheRelatedTo:\n",
    "        return cacheRelatedTo[word], True\n",
    "    baseUrl = \"https://api.conceptnet.io/query?node=/c/en/$WORD$&other=/c/en&limit=$LIMIT$&rel=/r/RelatedTo\"\n",
    "    processedURL = baseUrl.replace(\"$WORD$\", processedText)\n",
    "    processedURL = processedURL.replace(\"$LIMIT$\", str(limit))\n",
    "    #print(\"Processed URL: \", processedURL)\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = requests.get(processedURL).json()\n",
    "    except Exception:\n",
    "        return relatedTo, False\n",
    "    #obj = requests.get(processedURL).json()\n",
    "    if 'edges' not in obj:\n",
    "        return relatedTo, False\n",
    "    resultsLen = len(obj['edges'])\n",
    "    edges = obj['edges']\n",
    "    #print(\"Results:\", resultsLen)\n",
    "    for i in range(0, resultsLen):\n",
    "        edge = edges[i]\n",
    "        label = edge['end']['label']\n",
    "        relatedTo.add(label.lower())\n",
    "        label = edge['start']['label']\n",
    "        relatedTo.add(label.lower())\n",
    "    if dropWord:\n",
    "        relatedTo.discard(word.lower())\n",
    "    if useCache:\n",
    "        cacheRelatedTo[word] = relatedTo\n",
    "    return relatedTo, False\n",
    "\n",
    "def getIsA(word, limit=10, dropWord=True, useCache=True):\n",
    "    processedText = word.lower().replace(' ', '_')\n",
    "    isA = set()\n",
    "    if (len(processedText) <= 1):\n",
    "        return isA, True\n",
    "    if useCache and word in cacheIsA:\n",
    "        return cacheIsA[word], True\n",
    "    baseUrl = \"https://api.conceptnet.io/query?node=/c/en/$WORD$&other=/c/en&limit=$LIMIT$&rel=/r/IsA\"\n",
    "    processedURL = baseUrl.replace(\"$WORD$\", processedText)\n",
    "    processedURL = processedURL.replace(\"$LIMIT$\", str(limit))\n",
    "    #print(\"Processed URL: \", processedURL)\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = requests.get(processedURL).json()\n",
    "    except Exception:\n",
    "        return isA, False\n",
    "    #obj = requests.get(processedURL).json()\n",
    "    if 'edges' not in obj:\n",
    "        return isA, False\n",
    "    resultsLen = len(obj['edges'])\n",
    "    edges = obj['edges']\n",
    "    #print(\"Results:\", resultsLen)\n",
    "    for i in range(0, resultsLen):\n",
    "        edge = edges[i]\n",
    "        label = edge['end']['label']\n",
    "        isA.add(label.lower())\n",
    "    if dropWord:\n",
    "        isA.discard(word.lower())\n",
    "    if useCache:\n",
    "        cacheIsA[word] = isA\n",
    "    return isA, False\n",
    "\n",
    "def getDerivedFrom(word, limit=10, dropWord=True, useCache=True):\n",
    "    processedText = word.lower().replace(' ', '_')\n",
    "    derivedFrom = set()\n",
    "    if (len(processedText) <= 1):\n",
    "        return derivedFrom, True\n",
    "    if useCache and word in cacheDerivedFrom:\n",
    "        return cacheDerivedFrom[word], True\n",
    "    baseUrl = \"https://api.conceptnet.io/query?node=/c/en/$WORD$&other=/c/en&limit=$LIMIT$&rel=/r/DerivedFrom\"\n",
    "    processedURL = baseUrl.replace(\"$WORD$\", processedText)\n",
    "    processedURL = processedURL.replace(\"$LIMIT$\", str(limit))\n",
    "    #print(\"Processed URL: \", processedURL)\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = requests.get(processedURL).json()\n",
    "    except Exception:\n",
    "        return derivedFrom, False\n",
    "    #obj = requests.get(processedURL).json()\n",
    "    if 'edges' not in obj:\n",
    "        return derivedFrom, False\n",
    "    resultsLen = len(obj['edges'])\n",
    "    edges = obj['edges']\n",
    "    #print(\"Results:\", resultsLen)\n",
    "    for i in range(0, resultsLen):\n",
    "        edge = edges[i]\n",
    "        label = edge['end']['label']\n",
    "        if (label != word):\n",
    "            derivedFrom.add(label.lower())\n",
    "    if dropWord:\n",
    "        derivedFrom.discard(word.lower())\n",
    "    if useCache:\n",
    "        cacheDerivedFrom[word] = derivedFrom\n",
    "    return derivedFrom, False\n",
    "\n",
    "## LCS ##\n",
    "def getAmbiguousWithLCS(useStemming, col1, columns):\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    ambiguousValues = set()     \n",
    "    for col2 in columns:\n",
    "        if (col1 != col2):\n",
    "            lcs = getLCS(col1, col2)\n",
    "            minLen = min(len(col1), len(col2))\n",
    "            maxLen = max(len(col1), len(col2))\n",
    "            if (2 * minLen) < maxLen:\n",
    "                continue\n",
    "            if (len(lcs) >= (0.5*minLen) and len(lcs) > 1):\n",
    "                lcs = lcs.translate(translate_table)\n",
    "                if (lcs.lower() in stopwords.words('english')):\n",
    "                    continue\n",
    "                if (useStemming):\n",
    "                    words_in_lcs = word_tokenize(lcs)\n",
    "                    lcs_stemmed = \"\"\n",
    "                    for w in words_in_lcs:\n",
    "                        stemmedW = ps.stem(w)\n",
    "                        if (stemmedW in stemmed_words_list):\n",
    "                            lcs_stemmed += stemmedW + \" \"\n",
    "                    if (len(lcs_stemmed) > 0):\n",
    "                        #print(col1, col2, lcs, lcs_stemmed, sep=\"\\t\")\n",
    "                        ambiguousValues.add(lcs)\n",
    "                else:\n",
    "                    #print(col1, col2, lcs, sep=\"\\t\")\n",
    "                    ambiguousValues.add(lcs)\n",
    "    return ambiguousValues\n",
    "\n",
    "## WIKIPEDIA ##\n",
    "def getAmbiguityFromWikipedia(column, useStemming=True, results=2, useCache=True):\n",
    "        if useCache and column in cacheWikipedia:\n",
    "            return cacheWikipedia[column], True\n",
    "        wikipediaResults = set()\n",
    "        try:\n",
    "            wikipediaResults = set(wikipedia.search(column.replace(\"-\",\" \"), results))\n",
    "            wikipediaResultsStrip = set()\n",
    "            for result in wikipediaResults:\n",
    "                wikipediaResultsStrip.add(result.lower().strip())\n",
    "            wikipediaResults = wikipediaResultsStrip\n",
    "        except Exception:\n",
    "                pass\n",
    "        if (useStemming):\n",
    "            translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "            columnNoPunct = column.translate(translate_table)\n",
    "            stemmed_columns = [ps.stem(w) for w in word_tokenize(columnNoPunct)]\n",
    "            wikipediaResultsStemmed = set()\n",
    "            for wikipediaResult in wikipediaResults:\n",
    "                wikipediaResult = wikipediaResult.translate(translate_table)\n",
    "                words_in_wiki = word_tokenize(wikipediaResult)\n",
    "                stemmed_wiki = \"\"\n",
    "                for w in words_in_wiki:\n",
    "                    w_stemmed = ps.stem(w)\n",
    "                    if w_stemmed in stemmed_columns:\n",
    "                        stemmed_wiki = w + \" \"\n",
    "                if (len(stemmed_wiki) > 0):\n",
    "                    wikipediaResultsStemmed.add(stemmed_wiki.lower().strip())\n",
    "            if useCache:\n",
    "                cacheWikipedia[column] = wikipediaResultsStemmed\n",
    "            return wikipediaResultsStemmed, True\n",
    "        else:\n",
    "            if useCache:\n",
    "                cacheWikipedia[column] = wikipediaResults\n",
    "            return wikipediaResults, False\n",
    "\n",
    "## FUNCTIONS ##\n",
    "\n",
    "def getLabel(columns, limit=10, useStemming=True, useLCS=False):\n",
    "    labels = {}\n",
    "    provenance = {}\n",
    "    for column in columns:\n",
    "        provenanceMap = {}\n",
    "        #start_time = time.time()\n",
    "        synonyms, cachedSynonyms = getSynonym(column.replace(\"-\",\" \"), limit=limit, dropWord=True, useCache=True)\n",
    "        #print(\"Synonyms time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        relatedTo, cachedrelatedTo = getRelatedTo(column.replace(\"-\",\" \"), limit=limit, dropWord=True, useCache=True)\n",
    "        #print(\"RelatedTo time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        isA, cachedIsA = getIsA(column.replace(\"-\",\" \"), limit=limit, dropWord=True, useCache=True)\n",
    "        #print(\"IsA time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        derivedFrom, cachedDerivedFrom = getDerivedFrom(column.replace(\"-\",\" \"), limit=limit, dropWord=True, useCache=True)\n",
    "        #print(\"DerivedFrom time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        #wikipediaResults = set(wikipedia.search(column.replace(\"-\",\" \"), results=2))\n",
    "        wikipediaResults, cachedWikipediaResults = getAmbiguityFromWikipedia(column.replace(\"-\",\" \"), useStemming=True, results=2, useCache=True)\n",
    "        #print(\"Wikipedia time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        lcsAmb = getAmbiguousWithLCS(useStemming, column, columns)\n",
    "        #print(\"LSC time: %s\" %(time.time() - start_time))\n",
    "        labelsForColumn = set()\n",
    "        if len(synonyms) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(synonyms)\n",
    "        if len(relatedTo) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(relatedTo)\n",
    "        if len(wikipediaResults) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(wikipediaResults)\n",
    "        if len(isA) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(isA)\n",
    "        if len(derivedFrom) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(derivedFrom)\n",
    "        #labelsForColumn = synonyms | relatedTo | wikipediaResults | isA | derivedFrom\n",
    "        if len(labelsForColumn) == 0:\n",
    "            labelsForColumn = labelsForColumn | lcsAmb\n",
    "        if useLCS:\n",
    "            labelsForColumn = labelsForColumn | lcsAmb\n",
    "        provenanceMap['synonyms'] = synonyms\n",
    "        provenanceMap['relatedTo'] = relatedTo\n",
    "        provenanceMap['isA'] = isA\n",
    "        provenanceMap['derivedFrom'] = derivedFrom\n",
    "        provenanceMap['wikipediaResults'] = wikipediaResults\n",
    "        provenanceMap['lcsAmb'] = lcsAmb\n",
    "        labelsForColumn = [w.lower().strip() for w in list(labelsForColumn)]\n",
    "        labels[column] = set(labelsForColumn)\n",
    "        provenance[column] = provenanceMap\n",
    "        if not (cachedSynonyms or cachedrelatedTo or cachedIsA or cachedDerivedFrom):\n",
    "            time.sleep(1)\n",
    "    return labels, provenance\n",
    "\n",
    "def getAmbiguousColumns(columns, aliasDict):\n",
    "    colSet = set()\n",
    "    ambiguousLabels = {}\n",
    "    ambiguousAttr = {}\n",
    "    for col1 in columns:\n",
    "        for col2 in columns:\n",
    "            if col1 != col2:\n",
    "                alias1 = aliasDict[col1]\n",
    "                alias2 = aliasDict[col2]\n",
    "                ambiguousValues = set(alias1).intersection(set(alias2))\n",
    "                if (len(ambiguousValues) > 0):\n",
    "                    colSet.add(col1)\n",
    "                    colSet.add(col2)\n",
    "                    if col1 not in ambiguousLabels:\n",
    "                        ambiguousLabels[col1] = set(ambiguousValues)\n",
    "                    else:\n",
    "                        setValue = ambiguousLabels[col1]\n",
    "                        setValue = setValue | ambiguousValues\n",
    "                        ambiguousLabels[col1] = setValue\n",
    "                    if col2 not in ambiguousLabels:\n",
    "                        ambiguousLabels[col2] = set(ambiguousValues)\n",
    "                    else:\n",
    "                        setValue = ambiguousLabels[col2]\n",
    "                        setValue = setValue | ambiguousValues\n",
    "                        ambiguousLabels[col2] = setValue\n",
    "                    if col1 not in ambiguousAttr:\n",
    "                        setAttr = set()\n",
    "                        ambiguousAttr[col1] = setAttr\n",
    "                    setAttr = ambiguousAttr[col1]\n",
    "                    setAttr.add(col2)\n",
    "    return colSet, ambiguousLabels, ambiguousAttr\n",
    "\n",
    "def updateAlias(dic, dicAdd, dicRemove, blackList = set()):\n",
    "    for key, value in dicAdd.items():\n",
    "        if key not in dic:\n",
    "            dic[key] = set(value)\n",
    "        else:\n",
    "            setValue = dic[key]\n",
    "            for v in value:\n",
    "                setValue.add(v)\n",
    "    for key, value in dicRemove.items():\n",
    "        if key in dic:\n",
    "            setValue = dic[key]\n",
    "            for v in value:\n",
    "                setValue.discard(v)\n",
    "    #print(\"BLACKLIST:\")\n",
    "    #print(blackList)\n",
    "    for key, value in dic.items():\n",
    "        #print(\"BEFORE:\")\n",
    "        #print(value)\n",
    "        value = value - blackList\n",
    "        dic[key] = value\n",
    "        #print(\"AFTER:\")\n",
    "        #print(value)\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "def getStatsForValues(df, columns): \n",
    "    numerical = {}\n",
    "    categorical = {}\n",
    "    text = {}\n",
    "    for column in columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors='ignore')\n",
    "        columnType = df[column].dtype\n",
    "        if (columnType in numerics):\n",
    "            stats = df[column].describe()\n",
    "            min = stats['min']\n",
    "            max = stats['max']\n",
    "            mean = stats['mean']\n",
    "            median = stats['50%']\n",
    "            std = stats['std']\n",
    "            stats = (min, max, mean, median, std)\n",
    "            numerical[column] = stats\n",
    "        else:\n",
    "            lenValue = df[column].nunique()\n",
    "            lenColumn = df[column].shape[0]\n",
    "            threshold = lenColumn * 0.9\n",
    "            if (lenValue < threshold):\n",
    "                #compute frequencies\n",
    "                freq = df[column].value_counts()\n",
    "                categorical[column] = freq\n",
    "            else:\n",
    "                mean = df[column].map(len).mean()\n",
    "                std = df[column].map(len).std()\n",
    "                median = df[column].map(len).median()\n",
    "                tmpCol = df[column].sort_values()\n",
    "                min = tmpCol.iloc[0]\n",
    "                max = tmpCol.iloc[-1]\n",
    "                stats = (min, max, mean, median, std)\n",
    "                numerical[column] = stats\n",
    "\n",
    "    return numerical, categorical, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef15d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAmbiguousForCategorical(categorical):\n",
    "    colSet = set()\n",
    "    ambiguousValuesForAttr = {}\n",
    "    setPairs = {}\n",
    "    for c1, freq1 in categorical.items():\n",
    "        for c2, freq2 in categorical.items():\n",
    "            if (c1 != c2):\n",
    "                keys1 = freq1.keys()\n",
    "                keys2 = freq2.keys()\n",
    "                ambiguousValues = set(keys1).intersection(set(keys2))\n",
    "                if (len(ambiguousValues) < 1):\n",
    "                    continue\n",
    "                colSet.add(c1)\n",
    "                colSet.add(c2)\n",
    "                if c1 not in ambiguousValuesForAttr:\n",
    "                    ambiguousValuesForAttr[c1] = set(ambiguousValues)\n",
    "                else:\n",
    "                    setValue = ambiguousValuesForAttr[c1]\n",
    "                    setValue = setValue | ambiguousValues\n",
    "                    ambiguousValuesForAttr[c1] = setValue\n",
    "                if c1 not in setPairs:\n",
    "                    attrs = set()\n",
    "                    attrs.add(c2)\n",
    "                    setPairs[c1] = attrs\n",
    "                else:\n",
    "                    attrs = setPairs[c1]\n",
    "                    attrs.add(c2)\n",
    "    return colSet, ambiguousValuesForAttr, setPairs\n",
    "\n",
    "def getIntersection(min1, max1, min2, max2):\n",
    "    ## check type\n",
    "    if (type(min1) == str) or (type(max1) == str) or (type(min2) == str) or (type(max2) == str):\n",
    "        return None\n",
    "    ## check intervals\n",
    "    if (max1 < min1) or (max2 < min2):\n",
    "        print(\"Error min1-max1: {}-{} min2-max2:{}-{}\".format(min1, max1, min2, max2))\n",
    "        return None\n",
    "    # case min1 - max1, min2 - max2 --> no intersection\n",
    "    if (max1 < min2):\n",
    "        return None\n",
    "    # case min2 - max2, min1 - max2 --> no intersection\n",
    "    if (max2 < min1):\n",
    "        return None\n",
    "    # case min1, min2, max1, max2 --> min2-max1\n",
    "    if (min1 <= min2) and (min2 <= max1) and (max1 <= max2):\n",
    "        return (min2, max1)\n",
    "    # case min2, min1, max2, max1 --> min2-max1\n",
    "    if (min2 <= min1) and (min1 <= max2) and (max2 <= max1):\n",
    "        return (min1, max2)\n",
    "    # case min1, min2, max2, max1 --> min2-max2\n",
    "    if (min1 <= min2) and (min2 <= max2) and (max2 <= max1):\n",
    "        return (min2, max2)\n",
    "    # case min2, min1, max1, max2 --> min1-max1\n",
    "    return (min1, max1)\n",
    "\n",
    "\n",
    "def findAmbiguousForNumerical(numerical):\n",
    "    ## naive strategy: intersection of intervals\n",
    "    colSet = set()\n",
    "    ambiguousValuesForAttr = {}\n",
    "    setPairs = {}\n",
    "    for c1, stats1 in numerical.items():\n",
    "        for c2, stats2 in numerical.items():\n",
    "            if (c1 != c2):\n",
    "                interval = getIntersection(stats1[0], stats1[1], stats2[0], stats2[1])\n",
    "                if (interval is None):\n",
    "                    continue\n",
    "                colSet.add(c1)\n",
    "                colSet.add(c2)\n",
    "                if c1 not in ambiguousValuesForAttr:\n",
    "                    listValue = []\n",
    "                    listValue.append(interval)\n",
    "                    ambiguousValuesForAttr[c1] = listValue\n",
    "                else:\n",
    "                    listValue = ambiguousValuesForAttr[c1]\n",
    "                    listValue.append(interval)\n",
    "                if c1 not in setPairs:\n",
    "                    attrs = set()\n",
    "                    attrs.add(c2)\n",
    "                    setPairs[c1] = attrs\n",
    "                else:\n",
    "                    attrs = setPairs[c1]\n",
    "                    attrs.add(c2)\n",
    "    return colSet, ambiguousValuesForAttr, setPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4260e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAmbiguousForLabelAndData(setPairs, setPairsCategorical, setPairsNumerical, ambiguousValuesForAttrCategorical, ambiguousValuesForAttrNumerical):\n",
    "    setPairsLabelData = {}\n",
    "    ambiguousValues = {}\n",
    "    for attributeLabel, ambiguousAttributesLabel in setPairs.items():\n",
    "        setAmb = set()\n",
    "        for attributeCategorical, ambiguousAttributesCategorical in setPairsCategorical.items():\n",
    "            if (attributeLabel == attributeCategorical):\n",
    "                intersection = ambiguousAttributesLabel.intersection(ambiguousAttributesCategorical)\n",
    "                #intersection = list(intersection)\n",
    "                if (len(intersection) > 0):\n",
    "                    for amb in intersection:\n",
    "                        setAmb.add(amb)\n",
    "                    for attr in intersection:\n",
    "                        valuesAmb = ambiguousValuesForAttrCategorical[attr]\n",
    "                        for value in valuesAmb:\n",
    "                            if value in ambiguousValues:\n",
    "                                setAttrs = ambiguousValues[value]\n",
    "                                setAttrs.add(attr)\n",
    "                            else:\n",
    "                                setAttrs = set()\n",
    "                                setAttrs.add(attr)\n",
    "                                ambiguousValues[value] = setAttrs\n",
    "        for attributeNumerical, ambiguousAttributesNumerical in setPairsNumerical.items():\n",
    "            if (attributeLabel == attributeNumerical):\n",
    "                intersection = ambiguousAttributesLabel.intersection(ambiguousAttributesNumerical)\n",
    "                #intersection = list(intersection)\n",
    "                if (len(intersection) > 0):\n",
    "                    for amb in intersection:\n",
    "                        setAmb.add(amb)\n",
    "                    print(intersection)\n",
    "                    for attr in intersection:\n",
    "                        intervals = ambiguousValuesForAttrNumerical[attr]\n",
    "                        for interval in intervals:\n",
    "                            minI = interval[0]\n",
    "                            maxI = interval[1]\n",
    "                            if minI in ambiguousValues:\n",
    "                                setAttrs = ambiguousValues[minI]\n",
    "                                setAttrs.add(attr)\n",
    "                            else:\n",
    "                                setAttrs = set()\n",
    "                                setAttrs.add(attr)\n",
    "                                ambiguousValues[minI] = setAttrs\n",
    "                            if maxI in ambiguousValues:\n",
    "                                setAttrs = ambiguousValues[maxI]\n",
    "                                setAttrs.add(attr)\n",
    "                            else:\n",
    "                                setAttrs = set()\n",
    "                                setAttrs.add(attr)\n",
    "                                ambiguousValues[maxI] = setAttrs\n",
    "        setPairsLabelData[attributeLabel] = setAmb\n",
    "    return setPairsLabelData, ambiguousValues\n",
    "\n",
    "def reversedMap(map):\n",
    "    reversed = {}\n",
    "    for attrName, values in map.items():\n",
    "        for value in values:\n",
    "            #print(attrName, value)\n",
    "            if (value not in reversed):\n",
    "                    attrSet = set()\n",
    "                    attrSet.add(attrName)\n",
    "                    reversed[value] = attrSet\n",
    "            else:\n",
    "                    attrSet = reversed[value]\n",
    "                    attrSet.add(attrName)\n",
    "            #if type(value) is tuple:\n",
    "            #    print(\"TUPLE\")    \n",
    "    return reversedMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b07f82",
   "metadata": {},
   "source": [
    "# Data retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07857c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WDC download\n",
    "## one time execution\n",
    "\n",
    "skipDownload = True\n",
    "if skipDownload == False:\n",
    "    #for i in range(0, 25):\n",
    "    for i in range(0, 2):    \n",
    "        s = str(i)\n",
    "        if i < 10:\n",
    "            s = '0' + str(i)\n",
    "        url = \"http://data.dws.informatik.uni-mannheim.de/webtables/2015-07/englishCorpus/compressed/\" + s + \".tar.gz\"\n",
    "        response = requests.get(url, stream=True)\n",
    "        file = tarfile.open(fileobj=response.raw, mode=\"r|gz\")\n",
    "        path = \"./\" + s + \"/\"\n",
    "        file.extractall(path=path)\n",
    "        file.close()\n",
    "        tarFile = \"./\"+s+\"/\"+s+\".tar\"\n",
    "        !python -m tarfile -e {tarFile} ./WDC/\n",
    "\n",
    "        #os.remove(path)\n",
    "        os.remove(tarFile)\n",
    "        #os.remove('./WDC/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WDC Web Table Corpus 2015\n",
    "## All data\n",
    "relationalTables = []\n",
    "\n",
    "path_to_json = './WDC/0'\n",
    "#path_to_json = './WDC/1'\n",
    "#path_to_json = './WDC/2'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "\n",
    "start_time = time.time()\n",
    "for json_file in json_files:\n",
    "    path = path_to_json + \"/\" + json_file\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        if (data['tableType'] == 'RELATION') and (data['hasHeader'] == True) and (data['tableOrientation'] == \"HORIZONTAL\") and (data['headerPosition'] == \"FIRST_ROW\"):\n",
    "            relationalTables.append(data)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Loaded Relational JSON Files:\", len(relationalTables))\n",
    "print(\"Loaded in \", elapsed_time)\n",
    "\n",
    "#parsedSchemas = {}\n",
    "parsedSchemas = []\n",
    "start_time = time.time()\n",
    "for table in relationalTables:\n",
    "#for table in relationalTables[:5000]:\n",
    "    relationData = table['relation']\n",
    "    #pageTitle = table['pageTitle'].strip()\n",
    "    #title = table['title'].strip()\n",
    "    #print(\"PageTitle:\", pageTitle)\n",
    "    #print(\"Title:\",title)\n",
    "    columns = []\n",
    "    data = {}\n",
    "    for column in relationData:\n",
    "        columnName = column[0].strip()\n",
    "        if columnName != '':\n",
    "            columns.append(columnName)\n",
    "            data[columnName] = column[1:]\n",
    "    if len(data) > 0:\n",
    "        t = (columns, data)\n",
    "        parsedSchemas.append(t)\n",
    "\n",
    "print(\"Parsed schemas: \", len(parsedSchemas))\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Parsed in \", elapsed_time)\n",
    "del relationalTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97948a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "structuredData = []\n",
    "#K = 1500000\n",
    "#K = 1000\n",
    "#K = 5000\n",
    "#K = 10000\n",
    "#K = 25000\n",
    "#K = 50000\n",
    "#K = 75000\n",
    "#K = 100000\n",
    "#K = 150000\n",
    "K = 200000\n",
    "#K = 400000\n",
    "#K = 500000\n",
    "#K = 750000\n",
    "#K = 1000000\n",
    "#K = 1500000\n",
    "\n",
    "stats = {\"tables\": 0, \"columns\": 0, 'synonyms':0, 'relatedTo':0, 'isA':0, 'derivedFrom':0, 'wikipediaResults':0, 'lcsAmb':0}\n",
    "\n",
    "#for table, schemaAndData in parsedSchemas.items():\n",
    "#for schemaAndData in parsedSchemas:\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for schemaAndData in parsedSchemas:\n",
    "    columns = schemaAndData[0]\n",
    "    #print(columns)\n",
    "    aliasDict, provenance, colsToFind = findFromCached(cachedAlias, cachedProvenance, columns)\n",
    "    if (len(colsToFind) > 0):\n",
    "        #search online\n",
    "        #print(\"Search online since not stored in the cache for alias\")\n",
    "        aliasDictOnline, provenanceOnline = getLabel(colsToFind)\n",
    "        #print(aliasDictOnline)\n",
    "        #print(provenanceOnline)\n",
    "        aliasDict.update(aliasDictOnline)\n",
    "        provenance.update(provenanceOnline)\n",
    "        cachedAlias.update(aliasDict)\n",
    "        cachedProvenance.update(provenance)\n",
    "    #aliasDict, provenance = getLabel(columns)\n",
    "    #print(aliasDict)\n",
    "    #print(provenance)\n",
    "    ## TODO: periodically save caches\n",
    "    colSet, ambiguousLabels, setPairs = getAmbiguousColumns(columns, aliasDict)\n",
    "    #print(colSet)\n",
    "    #print(ambiguousLabels)\n",
    "    #print(setPairs)\n",
    "    #print(provenance)\n",
    "    colAmbiguous = list(colSet)\n",
    "    count += 1\n",
    "    if (count % 100) == 0:\n",
    "        print(\"Processed: \", count)\n",
    "        print(\"Elapsed time: %s\" %(time.time() - start_time))\n",
    "        print(\"StructuredData size: \", len(structuredData))\n",
    "    if (len(colSet) > 0):\n",
    "        stats['tables'] = stats['tables'] + 1\n",
    "        stats['columns'] = stats['columns'] + len(columns)\n",
    "        table = schemaAndData[1]\n",
    "        t = (table, columns, setPairs, ambiguousLabels, provenance)\n",
    "        structuredData.append(t)\n",
    "    if len(structuredData) == K:\n",
    "        break\n",
    "print(\"Process ended in time: %s\" %(time.time() - start_time))\n",
    "print(\"Structured data: \", len(structuredData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b203667",
   "metadata": {},
   "outputs": [],
   "source": [
    "## caches\n",
    "start_time = time.time()\n",
    "saveCache('./cacheSynonym-small.json', cacheSynonym)\n",
    "saveCache('./cacheRelatedTo-small.json', cacheRelatedTo)\n",
    "saveCache('./cacheIsA-small.json', cacheIsA)\n",
    "saveCache('./cacheDerivedFrom-small.json', cacheDerivedFrom)\n",
    "saveCache('./cacheWikipedia-small.json', cacheWikipedia)\n",
    "saveCache('./cacheAlias.json', cachedAlias)\n",
    "total_time = time.time() - start_time\n",
    "print(\"Process ended in time:\", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f0462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_file = open(\"cacheProvenance.pkl\", \"wb\")\n",
    "pickle.dump(cachedProvenance, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbccff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRow(schemaString, attr1, attr2, valueString):\n",
    "    return schemaString + \" attr1: \" + attr1 + \" attr2: \" + attr2 + \"\\t\" + valueString+\"\\n\"\n",
    "\n",
    "start_time = time.time()\n",
    "examples = []\n",
    "distinctLabels = set()\n",
    "distinctForType = {}\n",
    "for t in structuredData:\n",
    "    table = t[0]\n",
    "    columns = t[1]\n",
    "    setPairs = t[2]\n",
    "    ambiguousLabels = t[3]\n",
    "    provenance = t[4]\n",
    "    schemaString = \"|\".join(columns)\n",
    "    #TODO: implement it\n",
    "    #schemaString = generateFromData(table, columns)\n",
    "    countGenerated = 0\n",
    "    for attr1, attrSet in setPairs.items():\n",
    "        for attr2 in attrSet:\n",
    "            amb1 = ambiguousLabels[attr1]\n",
    "            amb2 = ambiguousLabels[attr2]\n",
    "            ambValues = set(amb1).intersection(set(amb2))\n",
    "            ambValues.discard(attr1.lower())\n",
    "            ambValues.discard(attr2.lower())\n",
    "            prov1 = provenance[attr1]\n",
    "            prov2= provenance[attr2]\n",
    "            for ambValue in ambValues:\n",
    "                distinctLabels.add(ambValue)\n",
    "                for key, valueSet in prov1.items():\n",
    "                    if ambValue in valueSet:\n",
    "                        stats[key] = stats[key] + 1\n",
    "                        if key not in distinctForType:\n",
    "                            distinctForType[key] = set()\n",
    "                        distLabelType = distinctForType[key]\n",
    "                        distLabelType.add(ambValue)\n",
    "                for key, valueSet in prov2.items():\n",
    "                    if ambValue in valueSet:\n",
    "                        stats[key] = stats[key] + 1\n",
    "                        if key not in distinctForType:\n",
    "                            distinctForType[key] = set()\n",
    "                        distLabelType = distinctForType[key]\n",
    "                        distLabelType.add(ambValue)\n",
    "                example = generateRow(schemaString, attr1, attr2, ambValue)\n",
    "                examples.append(example)\n",
    "                countGenerated += 1\n",
    "    for i in range(0, countGenerated):\n",
    "        #TODO improve the column selection\n",
    "        attr1 = choice(list(columns))\n",
    "        attr2 = choice(list(columns))\n",
    "        if (attr1 not in setPairs):\n",
    "            example = generateRow(schemaString, attr1, attr2, \"None\")\n",
    "            examples.append(example)\n",
    "        else:\n",
    "            attr1AmbSet = setPairs[attr1]\n",
    "            if ((attr2 != attr1) and (attr2 not in attr1AmbSet)):\n",
    "                example = generateRow(schemaString, attr1, attr2, \"None\")\n",
    "                examples.append(example)\n",
    "                \n",
    "print(stats)\n",
    "print(\"**** DISTINCT *****\")\n",
    "for key, value in distinctForType.items():\n",
    "    print(key, len(value))\n",
    "print(\"Unique labels: \", len(distinctLabels))\n",
    "total_time = time.time() - start_time\n",
    "print(\"Process ended in time:\", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d355a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Examples:\", len(examples))\n",
    "examplesNoDuplicates = set(examples)\n",
    "print(\"Examples no duplicates:\", len(examplesNoDuplicates))\n",
    "start_time = time.time()\n",
    "fileName = \"./generated/trainWDC-00-\" + str(K) + \"-v1.2.tsv\"\n",
    "#f = open(\"./trainWDC-00-5k-v1.1.tsv\", \"w\")\n",
    "f = open(fileName, \"w\")\n",
    "#f.writelines(examples)\n",
    "f.writelines(examplesNoDuplicates)\n",
    "f.close()\n",
    "total_time = time.time() - start_time\n",
    "print(\"Process ended in time:\", total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
