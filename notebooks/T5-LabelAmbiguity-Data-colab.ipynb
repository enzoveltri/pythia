{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SIGMOD-T5-LabelAmbiguity-Task-3.ipynb","private_outputs":true,"provenance":[{"file_id":"1eR_eRF8sTUJkd4KPy4T7dOqtzltwun5f","timestamp":1631996746317},{"file_id":"1i5I3q3Rm2-0yMNeE8_LnPdwJV3yGEyHV","timestamp":1630364051716},{"file_id":"1y3Zr3uqeXtTq2uv_yAqTqFbxYyY32l8v","timestamp":1626091831900}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"xYh-IaN4C7Z1"},"source":["print(\"Installing dependencies...\")\n","%tensorflow_version 2.x\n","!pip install -q t5\n","\n","import functools\n","import os\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","\n","import t5\n","import t5.models\n","import seqio\n","\n","BASE_DIR = \"gs://pythia_t5\" #@param { type: \"string\" }\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BASE_DIR.\")\n","DATA_DIR = os.path.join(BASE_DIR, \"data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","ON_CLOUD = True\n","\n","\n","if ON_CLOUD:\n","  print(\"Setting up GCS access...\")\n","  import tensorflow_gcs_config\n","  from google.colab import auth\n","  # Set credentials for GCS reading/writing from Colab and TPU.\n","  TPU_TOPOLOGY = \"v2-8\"\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    TPU_ADDRESS = tpu.get_master()\n","    print('Running on TPU:', TPU_ADDRESS)\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","  auth.authenticate_user()\n","  tf.enable_eager_execution()\n","  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n","\n","tf.disable_v2_behavior()\n","\n","# Improve logging.\n","from contextlib import contextmanager\n","import logging as py_logging\n","\n","if ON_CLOUD:\n","  tf.get_logger().propagate = False\n","  py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vbBPtK4P5R3"},"source":["#MODELS_DIR +=\"_task_3_5_cr\"\n","#MODELS_DIR +=\"_task_3_10_cr\"\n","MODELS_DIR +=\"_task_3_5_rr\"\n","#MODELS_DIR +=\"_task_3_10_rr\"\n","#MODELS_DIR +=\"_task_3_5_rr_gen_labels_large\"\n","#MODELS_DIR +=\"_task_3_5_rr_gen_labels_small\"\n","#MODELS_DIR +=\"_task_3_5_cr_gen_labels_large\"\n","#MODELS_DIR +=\"_task_3_5_cr_gen_labels_small\"\n","#MODELS_DIR +=\"_task_3_10_rr_labels_small\"\n","print(\"DATA_DIR:\", DATA_DIR)\n","print(\"MODELS_DIR:\", MODELS_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpQt-PoGzSrY"},"source":["import json\n","print(DATA_DIR)\n","#counts_path = os.path.join(DATA_DIR, \"counts.json\")\n","tsv_path = {\n","    #\"train\": os.path.join(DATA_DIR, \"train-task3-5-cr-v1.2.tsv\"),\n","    \"train\": os.path.join(DATA_DIR, \"train-task3-5-rr-v1.2.tsv\"),\n","    #\"train\": os.path.join(DATA_DIR, \"train-task3-5-rr-v1.2_Gen_Labels_Large.tsv\"),\n","    #\"train\": os.path.join(DATA_DIR, \"train-task3-5-rr-v1.2_Gen_Labels_Small.tsv\"),\n","    #\"train\": os.path.join(DATA_DIR, \"train-task3-5-cr-v1.2_Gen_Labels_Small.tsv\"),\n","    #\"train\": os.path.join(DATA_DIR, \"train-task3-5-cr-v1.2_Gen_Labels_Large.tsv\"),\n","    #\"train\": os.path.join(DATA_DIR, \"train-task3-10-rr-v1.2_Gen_Labels_Small.tsv\"),\n","    #\"train\": os.path.join(DATA_DIR, \"train-task3-10-cr-v1.2.tsv\"),\n","    #\"train\": os.path.join(DATA_DIR, \"train-task3-10-rr-v1.2.tsv\"),\n","    \"validation\": os.path.join(DATA_DIR, \"test-task3-manual-5-rr.tsv\"),\n","    #\"validation\": os.path.join(DATA_DIR, \"test-task3-manual-10-cr.tsv\"),\n","    #\"validation\": os.path.join(DATA_DIR, \"test-task3-manual-5-cr.tsv\"),\n","    #\"validation\": os.path.join(DATA_DIR, \"test-task3-manual-10-rr.tsv\"),\n","    #\"validation\": os.path.join(DATA_DIR, \"test-task3-5-rr-strict.tsv\"), #\"test-task1-manual.tsv\"),\n","}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rotx6zBYILAf"},"source":["def to_dataset_ts(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(tsv_path[split])\n","  # Split each \"<question>\\t<answer>\" example into (question, answer) tuple.\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  # Map each tuple to a {\"totto\": ... \"explain_table\": ...} dict.\n","  ds = ds.map(lambda *ex: dict(zip([\"ambiguous\", \"label\"], ex)))\n","  return ds\n","\n","print(\"A few raw validation examples...\")\n","for ex in tfds.as_numpy(to_dataset_ts(\"validation\").take(5)):\n","  print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrEmmlAaXdWa"},"source":["def text_preprocessor(ds):\n","  def normalize_text(text):\n","    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n","    text = tf.strings.lower(text)\n","    #text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n","    return text\n","\n","  def to_inputs_and_targets(ex):\n","    \"\"\"Map {\"question\": ..., \"answer\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n","    return {\n","        \"inputs\":\n","             tf.strings.join(\n","                 [\"pythia_schema_task1: \", normalize_text(ex[\"ambiguous\"])]),\n","        \"targets\": normalize_text(ex[\"label\"])\n","    }\n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"faN9NGGVH_xq"},"source":["#!pip install t5[gcp]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgxYaEj5f9xy"},"source":["def extractValues(x):\n","  s = x.replace('[', '').replace(']','')\n","  if len(s) == 0:\n","      return None\n","  else:\n","      #print(s)\n","      splits = s.split(sep=',')\n","      values = []\n","      for split in splits:\n","          tmp = split.strip().replace(\"'\", \"\")\n","          values.append(tmp)\n","      return values\n","\n","def countStats(targets, predictions):\n","  tn = 0\n","  fp = 0\n","  tp = 0\n","  fn = 0\n","  wrongLabels = 0\n","  for tgt, pred in zip(targets, predictions):\n","    #print(tgt, pred)\n","    valuesTgt = extractValues(tgt)\n","    if valuesTgt is None:\n","      valuesTgt = [\"none\"]\n","    if \"none\" in valuesTgt:\n","      if pred == \"none\":\n","        #print(\"TN\")\n","        tn += 1\n","        continue\n","      else:\n","        fp += 1\n","        #print(\"FP\")\n","        continue\n","    if \"none\" not in valuesTgt:\n","      if pred == \"none\":\n","        fn += 1\n","        #print(\"FN\")\n","        continue\n","      else:\n","        ff = False\n","        for v in valuesTgt:\n","          if v == pred:\n","            ff = True\n","            tp += 1\n","            #print(\"TP\")\n","            break\n","        if ff == False:\n","          wrongLabels += 1\n","          #print(\"FN\")\n","  return tn, fp, tp, fn, wrongLabels\n","\n","def countStatsLabelsStats(targets, predictions):\n","    count_pairs = 0\n","    hits = 0\n","    #print(\"**** TARGETS *****\")\n","    #print(type(targets))\n","    #print(targets)\n","    #print(\"**** PREDICTIONS *****\")\n","    #print(type(predictions))\n","    #print(predictions)\n","    #return 0/0\n","    for tgt, pred in zip(targets, predictions):\n","        valuesTgt = extractValues(tgt)\n","        if valuesTgt is not None:\n","          count_pairs += 1\n","        if (pred != \"none\") and (valuesTgt is not None):\n","          for v in valuesTgt:\n","            if v == pred:\n","              ff = True\n","              hits += 1\n","              break\n","    return {\"count_pairs\": count_pairs, \"hits\": hits}\n","    #return count_pairs, hits\n","\n","def my_accuracy_fn(targets, predictions):\n","  hits = 0\n","  for tgt, pred in zip(targets, predictions):\n","    valuesTgt = extractValues(tgt)\n","    if valuesTgt is None:\n","      valuesTgt = [\"none\"]\n","    for v in valuesTgt:\n","      if v == pred:\n","        hits += 1\n","        continue\n","  total = len(targets)\n","  accuracy = hits/total\n","  return {\"my_accuracy\": accuracy}\n","\n","def my_precision_fn(targets, predictions):\n","  tn, fp, tp, fn, wrongLabels = countStats(targets, predictions)\n","  if tp + fp == 0:\n","    return {\"my_precision\": 0}\n","  precision = tp/(tp + fp)\n","  return {\"my_precision\": precision}\n","\n","def my_recall_fn(targets, predictions):\n","  tn, fp, tp, fn, wrongLabels = countStats(targets, predictions)\n","  if tp + fn == 0:\n","    return {\"my_recall\": 0}\n","  recall = tp/(tp + fn)\n","  return {\"my_recall\": recall}\n","\n","def my_recall_fn_2(targets, predictions):\n","  tn, fp, tp, fn, wrongLabels = countStats(targets, predictions)\n","  if tp + fn == 0:\n","    return {\"my_recall_2\": 0}\n","  recall = tp/(tp + fn + wrongLabels)\n","  return {\"my_recall_2\": recall}\n","\n","def my_precision_binary(targets, predictions):\n","  tn, fp, tp, fn, wrongLabels = countStats(targets, predictions)\n","  if tp + wrongLabels + fp == 0:\n","    return {\"my_precision_binary\": 0}\n","  precisionBinary = (tp + wrongLabels)/(tp + wrongLabels + fp)\n","  return {\"my_precision_binary\": precisionBinary}\n","\n","def my_recall_binary(targets, predictions):\n","  tn, fp, tp, fn, wrongLabels = countStats(targets, predictions)\n","  if tp + fn + wrongLabels == 0:\n","    return {\"my_recall_binary\": 0}\n","  recallBinary = (tp + wrongLabels) / (tp + fn + wrongLabels)\n","  return {\"my_recall_binary\": recallBinary}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeXtLWtqGRup"},"source":["#import t5\n","\n","t5.data.TaskRegistry.add(\n","    \"ambiguity_context\",\n","    # Specify the task type.\n","    t5.data.Task,\n","    # Supply a function which returns a tf.data.Dataset.\n","    dataset_fn=to_dataset_ts,\n","    splits=[\"train\", \"validation\"],\n","    # Supply a function which preprocesses text from the tf.data.Dataset.\n","    text_preprocessor=[text_preprocessor],\n","    # Lowercase targets before computing metrics.\n","    postprocess_fn=t5.data.postprocessors.lower_text, \n","    # We'll use accuracy as our evaluation metric.\n","    #metric_fns=[t5.evaluation.metrics.accuracy],\n","    metric_fns=[my_accuracy_fn, my_precision_fn, my_recall_fn, my_recall_fn_2, my_precision_binary, my_recall_binary, countStatsLabelsStats],\n","    # Not required, but helps for mixing and auto-caching.\n","    #num_input_examples=num_nq_examples\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTLhColtHGZf"},"source":["nq_task = t5.data.TaskRegistry.get(\"ambiguity_context\")\n","#ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 128, \"targets\": 32})\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 32})\n","#ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 1024, \"targets\": 32})\n","print(\"A few preprocessed train examples...\")\n","for ex in tfds.as_numpy(ds.take(5)):\n","  print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rr4m7ErRLo_D"},"source":["t5.data.MixtureRegistry.remove(\"all_mix\")\n","t5.data.MixtureRegistry.add(\n","    \"all_mix\",\n","    [\"ambiguity_context\"],\n","     default_rate=1.0\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yGQ-zpgy3raf"},"source":["MODEL_SIZE = \"3B\" #@param[\"small\", \"base\", \"large\", \"3B\", \"11B\"]\n","# Public GCS path for T5 pre-trained model checkpoints\n","BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n","PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n","MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)\n","\n","if ON_CLOUD and MODEL_SIZE == \"3B\":\n","  tf.logging.warning(\n","      \"The `3B` model is too large to use with the 5GB GCS free tier. \"\n","      \"Make sure you have at least 25GB on GCS before continuing.\"\n","  )\n","elif ON_CLOUD and MODEL_SIZE == \"11B\":\n","  raise ValueError(\n","      \"The `11B` parameter is too large to fine-tune on the `v2-8` TPU \"\n","      \"provided by Colab. Please comment out this Error if you're running \"\n","      \"on a larger TPU.\"\n","  )\n","\n","# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n","# Limit number of checkpoints to fit within 5GB (if possible).\n","model_parallelism, train_batch_size, keep_checkpoint_max = {\n","    \"small\": (1, 256, 16),\n","    \"base\": (2, 128, 8),\n","    \"large\": (8, 64, 4),\n","    \"3B\": (8, 16, 1),\n","    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n","\n","tf.io.gfile.makedirs(MODEL_DIR)\n","# The models from our paper are based on the Mesh Tensorflow Transformer.\n","\n","model = t5.models.MtfModel(\n","    model_dir=MODEL_DIR,\n","    tpu=TPU_ADDRESS,\n","    tpu_topology=TPU_TOPOLOGY,\n","    model_parallelism=model_parallelism,\n","    batch_size=train_batch_size,\n","    #sequence_length={\"inputs\": 128, \"targets\": 32},\n","    sequence_length={\"inputs\": 512, \"targets\": 32}, ## good for 5 encoding traing\n","    #sequence_length={\"inputs\": 1024, \"targets\": 32},\n","    learning_rate_schedule=0.003,\n","    save_checkpoints_steps=100,\n","    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n","    iterations_per_loop=100,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZjA7Tdtnk0x"},"source":["if ON_CLOUD:\n","  %reload_ext tensorboard\n","%tensorboard --logdir=\"$MODEL_DIR\" --port=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ry_XhxKp9qRf"},"source":["print(model._keep_checkpoint_max)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BkjfoGswNGqk"},"source":["FINETUNE_STEPS =  3000#@param {type: \"integer\"}\n","\n","model._save_checkpoints_steps=200\n","model._keep_checkpoint_max = 50\n","\n","model.finetune(\n","    mixture_or_task_name=\"all_mix\",\n","    pretrained_model_dir=PRETRAINED_DIR,\n","    finetune_steps=FINETUNE_STEPS\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkUZhtWNEosT"},"source":["print(model.batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0dH89bY3NPZ4"},"source":["#model.batch_size = train_batch_size * 4\n","model.eval(\n","    mixture_or_task_name=\"ambiguity_context\",\n","    #checkpoint_steps=\"all\",\n","    checkpoint_steps=[1000300, 1000600, 1000900, 1001200, 1001500, 1001800, 1002100, 1002400, 1002700, 1003000],\n","    #checkpoint_steps=[1000500,1001000,1001500, 1002000, 1002500, 1003000, 1003500, 1004000, 1004500, 1005000, 1006000, 1007000, 1008000, 1009000, 1010000],\n","    #checkpoint_steps = [1001500],\n","    summary_dir = MODEL_DIR\n",")"],"execution_count":null,"outputs":[]}]}