{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999d6047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/enzoveltri/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/enzoveltri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/enzoveltri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from wikipedia) (4.9.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from wikipedia) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (4.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from beautifulsoup4->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: pyinterval in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: crlibm==1.*,>=1.0.3 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from pyinterval) (1.0.3)\n",
      "Requirement already satisfied: six>=1.10 in /Users/enzoveltri/opt/anaconda3/envs/sentencegenerator/lib/python3.7/site-packages (from pyinterval) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from difflib import get_close_matches, SequenceMatcher\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from random import choice, randrange\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "!pip install wikipedia\n",
    "import wikipedia\n",
    "!pip install pyinterval\n",
    "import os\n",
    "import tarfile\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7097a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data prep\n",
    "ps = PorterStemmer()\n",
    "stemmed_words_list = [ps.stem(w) for w in words.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b9241c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils funct\n",
    "def saveCache(fileName, cache):\n",
    "    tempDict = {}\n",
    "    for key, value in cache.items():\n",
    "        tempDict[key] = list(value)\n",
    "    with open(fileName, 'w') as fp:\n",
    "        json.dump(tempDict, fp)\n",
    "        \n",
    "def loadCache(fileName):\n",
    "    data = None\n",
    "    with open(fileName, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    cache = {}\n",
    "    for key, value in data.items():\n",
    "        cache[key] = set(value)\n",
    "    return cache\n",
    "\n",
    "def findFromCached(cachedAlias, cachedProvenance, columns):\n",
    "    aliasReturn = {}\n",
    "    provenanceReturn = {}\n",
    "    columnsToSearch = set()\n",
    "    for column in columns:\n",
    "        if column in cachedAlias:\n",
    "            aliasReturn[column] = cachedAlias[column]\n",
    "        else:\n",
    "            columnsToSearch.add(column)\n",
    "        if column in cachedProvenance:\n",
    "            provenanceReturn[column] = cachedProvenance[column]\n",
    "        else:\n",
    "            columnsToSearch.add(column)\n",
    "     \n",
    "    return aliasReturn, provenanceReturn, columnsToSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e9f2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "## caches\n",
    "cacheSynonym = {}\n",
    "cacheRelatedTo = {}\n",
    "cacheIsA = {}\n",
    "cacheDerivedFrom = {}\n",
    "cacheWikipedia = {}\n",
    "cachedAlias = {}\n",
    "cachedProvenance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load caches  \n",
    "cacheSynonym = loadCache('./cacheSynonym-small.json')\n",
    "cacheRelatedTo = loadCache('./cacheRelatedTo-small.json')\n",
    "cacheIsA = loadCache('./cacheIsA-small.json')\n",
    "cacheDerivedFrom = loadCache('./cacheDerivedFrom-small.json')\n",
    "cacheWikipedia = loadCache('./cacheWikipedia-small.json')\n",
    "cachedAlias = loadCache('./cacheAlias.json')\n",
    "a_file = open(\"cacheProvenance.pkl\", \"rb\")\n",
    "cachedProvenance = pickle.load(a_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b6031f",
   "metadata": {},
   "source": [
    "## Annotator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6089780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLCS(string1, string2):\n",
    "    string1 = string1.lower()\n",
    "    string2 = string2.lower()\n",
    "    match = SequenceMatcher(None, string1, string2).find_longest_match(0, len(string1), 0, len(string2))\n",
    "    return string1[match.a: match.a + match.size].lower().strip()\n",
    "\n",
    "## CONCEPTNET.IO ##\n",
    "def getSynonym(word, limit=10, dropWord=True, useCache=True):\n",
    "    processedText = word.lower().replace(' ', '_')\n",
    "    synonyms = set()\n",
    "    if (len(processedText) <= 1):\n",
    "        return synonyms, True\n",
    "    if useCache and word in cacheSynonym:\n",
    "        return cacheSynonym[word], True\n",
    "    baseUrl = \"https://api.conceptnet.io/query?node=/c/en/$WORD$&other=/c/en&limit=$LIMIT$&rel=/r/Synonym\"\n",
    "    processedURL = baseUrl.replace(\"$WORD$\", processedText)\n",
    "    processedURL = processedURL.replace(\"$LIMIT$\", str(limit))\n",
    "    #print(\"Processed URL: \", processedURL)\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = requests.get(processedURL).json()\n",
    "    except Exception:\n",
    "        return synonyms, False\n",
    "    #obj = requests.get(processedURL).json()\n",
    "    if 'edges' not in obj:\n",
    "        return synonyms, False\n",
    "    resultsLen = len(obj['edges'])\n",
    "    edges = obj['edges']\n",
    "    #print(\"Results:\", resultsLen)\n",
    "    \n",
    "    for i in range(0, resultsLen):\n",
    "        edge = edges[i]\n",
    "        label = edge['end']['label']\n",
    "        synonyms.add(label.lower())\n",
    "        label = edge['start']['label']\n",
    "        synonyms.add(label.lower())\n",
    "    if dropWord:\n",
    "        synonyms.discard(word.lower())\n",
    "    if useCache:\n",
    "        cacheSynonym[word] = synonyms\n",
    "    return synonyms, False\n",
    "\n",
    "def getRelatedTo(word, limit=10, dropWord=True, useCache=True):\n",
    "    processedText = word.lower().replace(' ', '_')\n",
    "    relatedTo = set()\n",
    "    if (len(processedText) <= 1):\n",
    "        return relatedTo, True\n",
    "    if useCache and word in cacheRelatedTo:\n",
    "        return cacheRelatedTo[word], True\n",
    "    baseUrl = \"https://api.conceptnet.io/query?node=/c/en/$WORD$&other=/c/en&limit=$LIMIT$&rel=/r/RelatedTo\"\n",
    "    processedURL = baseUrl.replace(\"$WORD$\", processedText)\n",
    "    processedURL = processedURL.replace(\"$LIMIT$\", str(limit))\n",
    "    #print(\"Processed URL: \", processedURL)\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = requests.get(processedURL).json()\n",
    "    except Exception:\n",
    "        return relatedTo, False\n",
    "    #obj = requests.get(processedURL).json()\n",
    "    if 'edges' not in obj:\n",
    "        return relatedTo, False\n",
    "    resultsLen = len(obj['edges'])\n",
    "    edges = obj['edges']\n",
    "    #print(\"Results:\", resultsLen)\n",
    "    for i in range(0, resultsLen):\n",
    "        edge = edges[i]\n",
    "        label = edge['end']['label']\n",
    "        relatedTo.add(label.lower())\n",
    "        label = edge['start']['label']\n",
    "        relatedTo.add(label.lower())\n",
    "    if dropWord:\n",
    "        relatedTo.discard(word.lower())\n",
    "    if useCache:\n",
    "        cacheRelatedTo[word] = relatedTo\n",
    "    return relatedTo, False\n",
    "\n",
    "def getIsA(word, limit=10, dropWord=True, useCache=True):\n",
    "    processedText = word.lower().replace(' ', '_')\n",
    "    isA = set()\n",
    "    if (len(processedText) <= 1):\n",
    "        return isA, True\n",
    "    if useCache and word in cacheIsA:\n",
    "        return cacheIsA[word], True\n",
    "    baseUrl = \"https://api.conceptnet.io/query?node=/c/en/$WORD$&other=/c/en&limit=$LIMIT$&rel=/r/IsA\"\n",
    "    processedURL = baseUrl.replace(\"$WORD$\", processedText)\n",
    "    processedURL = processedURL.replace(\"$LIMIT$\", str(limit))\n",
    "    #print(\"Processed URL: \", processedURL)\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = requests.get(processedURL).json()\n",
    "    except Exception:\n",
    "        return isA, False\n",
    "    #obj = requests.get(processedURL).json()\n",
    "    if 'edges' not in obj:\n",
    "        return isA, False\n",
    "    resultsLen = len(obj['edges'])\n",
    "    edges = obj['edges']\n",
    "    #print(\"Results:\", resultsLen)\n",
    "    for i in range(0, resultsLen):\n",
    "        edge = edges[i]\n",
    "        label = edge['end']['label']\n",
    "        isA.add(label.lower())\n",
    "    if dropWord:\n",
    "        isA.discard(word.lower())\n",
    "    if useCache:\n",
    "        cacheIsA[word] = isA\n",
    "    return isA, False\n",
    "\n",
    "def getDerivedFrom(word, limit=10, dropWord=True, useCache=True):\n",
    "    processedText = word.lower().replace(' ', '_')\n",
    "    derivedFrom = set()\n",
    "    if (len(processedText) <= 1):\n",
    "        return derivedFrom, True\n",
    "    if useCache and word in cacheDerivedFrom:\n",
    "        return cacheDerivedFrom[word], True\n",
    "    baseUrl = \"https://api.conceptnet.io/query?node=/c/en/$WORD$&other=/c/en&limit=$LIMIT$&rel=/r/DerivedFrom\"\n",
    "    processedURL = baseUrl.replace(\"$WORD$\", processedText)\n",
    "    processedURL = processedURL.replace(\"$LIMIT$\", str(limit))\n",
    "    #print(\"Processed URL: \", processedURL)\n",
    "    obj = None\n",
    "    try:\n",
    "        obj = requests.get(processedURL).json()\n",
    "    except Exception:\n",
    "        return derivedFrom, False\n",
    "    #obj = requests.get(processedURL).json()\n",
    "    if 'edges' not in obj:\n",
    "        return derivedFrom, False\n",
    "    resultsLen = len(obj['edges'])\n",
    "    edges = obj['edges']\n",
    "    #print(\"Results:\", resultsLen)\n",
    "    for i in range(0, resultsLen):\n",
    "        edge = edges[i]\n",
    "        label = edge['end']['label']\n",
    "        if (label != word):\n",
    "            derivedFrom.add(label.lower())\n",
    "    if dropWord:\n",
    "        derivedFrom.discard(word.lower())\n",
    "    if useCache:\n",
    "        cacheDerivedFrom[word] = derivedFrom\n",
    "    return derivedFrom, False\n",
    "\n",
    "## LCS ##\n",
    "def getAmbiguousWithLCS(useStemming, col1, columns):\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    ambiguousValues = set()     \n",
    "    for col2 in columns:\n",
    "        if (col1 != col2):\n",
    "            lcs = getLCS(col1, col2)\n",
    "            minLen = min(len(col1), len(col2))\n",
    "            maxLen = max(len(col1), len(col2))\n",
    "            if (2 * minLen) < maxLen:\n",
    "                continue\n",
    "            if (len(lcs) >= (0.5*minLen) and len(lcs) > 1):\n",
    "                lcs = lcs.translate(translate_table)\n",
    "                if (lcs.lower() in stopwords.words('english')):\n",
    "                    continue\n",
    "                if (useStemming):\n",
    "                    words_in_lcs = word_tokenize(lcs)\n",
    "                    lcs_stemmed = \"\"\n",
    "                    for w in words_in_lcs:\n",
    "                        stemmedW = ps.stem(w)\n",
    "                        if (stemmedW in stemmed_words_list):\n",
    "                            lcs_stemmed += stemmedW + \" \"\n",
    "                    if (len(lcs_stemmed) > 0):\n",
    "                        #print(col1, col2, lcs, lcs_stemmed, sep=\"\\t\")\n",
    "                        ambiguousValues.add(lcs)\n",
    "                else:\n",
    "                    #print(col1, col2, lcs, sep=\"\\t\")\n",
    "                    ambiguousValues.add(lcs)\n",
    "    return ambiguousValues\n",
    "\n",
    "## WIKIPEDIA ##\n",
    "def getAmbiguityFromWikipedia(column, useStemming=True, results=2, useCache=True):\n",
    "        if useCache and column in cacheWikipedia:\n",
    "            return cacheWikipedia[column], True\n",
    "        wikipediaResults = set()\n",
    "        try:\n",
    "            wikipediaResults = set(wikipedia.search(column.replace(\"-\",\" \"), results))\n",
    "            wikipediaResultsStrip = set()\n",
    "            for result in wikipediaResults:\n",
    "                wikipediaResultsStrip.add(result.lower().strip())\n",
    "            wikipediaResults = wikipediaResultsStrip\n",
    "        except Exception:\n",
    "                pass\n",
    "        if (useStemming):\n",
    "            translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "            columnNoPunct = column.translate(translate_table)\n",
    "            stemmed_columns = [ps.stem(w) for w in word_tokenize(columnNoPunct)]\n",
    "            wikipediaResultsStemmed = set()\n",
    "            for wikipediaResult in wikipediaResults:\n",
    "                wikipediaResult = wikipediaResult.translate(translate_table)\n",
    "                words_in_wiki = word_tokenize(wikipediaResult)\n",
    "                stemmed_wiki = \"\"\n",
    "                for w in words_in_wiki:\n",
    "                    w_stemmed = ps.stem(w)\n",
    "                    if w_stemmed in stemmed_columns:\n",
    "                        stemmed_wiki = w + \" \"\n",
    "                if (len(stemmed_wiki) > 0):\n",
    "                    wikipediaResultsStemmed.add(stemmed_wiki.lower().strip())\n",
    "            if useCache:\n",
    "                cacheWikipedia[column] = wikipediaResultsStemmed\n",
    "            return wikipediaResultsStemmed, True\n",
    "        else:\n",
    "            if useCache:\n",
    "                cacheWikipedia[column] = wikipediaResults\n",
    "            return wikipediaResults, False\n",
    "\n",
    "## FUNCTIONS ##\n",
    "\n",
    "def getLabel(columns, limit=10, useStemming=True, useLCS=False):\n",
    "    labels = {}\n",
    "    provenance = {}\n",
    "    for column in columns:\n",
    "        provenanceMap = {}\n",
    "        #start_time = time.time()\n",
    "        synonyms, cachedSynonyms = getSynonym(column.replace(\"-\",\" \"), limit=limit, dropWord=True, useCache=True)\n",
    "        #print(\"Synonyms time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        relatedTo, cachedrelatedTo = getRelatedTo(column.replace(\"-\",\" \"), limit=limit, dropWord=True, useCache=True)\n",
    "        #print(\"RelatedTo time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        isA, cachedIsA = getIsA(column.replace(\"-\",\" \"), limit=limit, dropWord=True, useCache=True)\n",
    "        #print(\"IsA time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        derivedFrom, cachedDerivedFrom = getDerivedFrom(column.replace(\"-\",\" \"), limit=limit, dropWord=True, useCache=True)\n",
    "        #print(\"DerivedFrom time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        #wikipediaResults = set(wikipedia.search(column.replace(\"-\",\" \"), results=2))\n",
    "        wikipediaResults, cachedWikipediaResults = getAmbiguityFromWikipedia(column.replace(\"-\",\" \"), useStemming=True, results=2, useCache=True)\n",
    "        #print(\"Wikipedia time: %s\" %(time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        lcsAmb = getAmbiguousWithLCS(useStemming, column, columns)\n",
    "        #print(\"LSC time: %s\" %(time.time() - start_time))\n",
    "        labelsForColumn = set()\n",
    "        if len(synonyms) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(synonyms)\n",
    "        if len(relatedTo) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(relatedTo)\n",
    "        if len(wikipediaResults) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(wikipediaResults)\n",
    "        if len(isA) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(isA)\n",
    "        if len(derivedFrom) > 0:\n",
    "            labelsForColumn = labelsForColumn | set(derivedFrom)\n",
    "        #labelsForColumn = synonyms | relatedTo | wikipediaResults | isA | derivedFrom\n",
    "        if len(labelsForColumn) == 0:\n",
    "            labelsForColumn = labelsForColumn | lcsAmb\n",
    "        if useLCS:\n",
    "            labelsForColumn = labelsForColumn | lcsAmb\n",
    "        provenanceMap['synonyms'] = synonyms\n",
    "        provenanceMap['relatedTo'] = relatedTo\n",
    "        provenanceMap['isA'] = isA\n",
    "        provenanceMap['derivedFrom'] = derivedFrom\n",
    "        provenanceMap['wikipediaResults'] = wikipediaResults\n",
    "        provenanceMap['lcsAmb'] = lcsAmb\n",
    "        labelsForColumn = [w.lower().strip() for w in list(labelsForColumn)]\n",
    "        labels[column] = set(labelsForColumn)\n",
    "        provenance[column] = provenanceMap\n",
    "        if not (cachedSynonyms or cachedrelatedTo or cachedIsA or cachedDerivedFrom):\n",
    "            time.sleep(1)\n",
    "    return labels, provenance\n",
    "\n",
    "def getAmbiguousColumns(columns, aliasDict):\n",
    "    colSet = set()\n",
    "    ambiguousLabels = {}\n",
    "    ambiguousAttr = {}\n",
    "    for col1 in columns:\n",
    "        for col2 in columns:\n",
    "            if col1 != col2:\n",
    "                alias1 = aliasDict[col1]\n",
    "                alias2 = aliasDict[col2]\n",
    "                ambiguousValues = set(alias1).intersection(set(alias2))\n",
    "                if (len(ambiguousValues) > 0):\n",
    "                    colSet.add(col1)\n",
    "                    colSet.add(col2)\n",
    "                    if col1 not in ambiguousLabels:\n",
    "                        ambiguousLabels[col1] = set(ambiguousValues)\n",
    "                    else:\n",
    "                        setValue = ambiguousLabels[col1]\n",
    "                        setValue = setValue | ambiguousValues\n",
    "                        ambiguousLabels[col1] = setValue\n",
    "                    if col2 not in ambiguousLabels:\n",
    "                        ambiguousLabels[col2] = set(ambiguousValues)\n",
    "                    else:\n",
    "                        setValue = ambiguousLabels[col2]\n",
    "                        setValue = setValue | ambiguousValues\n",
    "                        ambiguousLabels[col2] = setValue\n",
    "                    if col1 not in ambiguousAttr:\n",
    "                        setAttr = set()\n",
    "                        ambiguousAttr[col1] = setAttr\n",
    "                    setAttr = ambiguousAttr[col1]\n",
    "                    setAttr.add(col2)\n",
    "    return colSet, ambiguousLabels, ambiguousAttr\n",
    "\n",
    "def updateAlias(dic, dicAdd, dicRemove, blackList = set()):\n",
    "    for key, value in dicAdd.items():\n",
    "        if key not in dic:\n",
    "            dic[key] = set(value)\n",
    "        else:\n",
    "            setValue = dic[key]\n",
    "            for v in value:\n",
    "                setValue.add(v)\n",
    "    for key, value in dicRemove.items():\n",
    "        if key in dic:\n",
    "            setValue = dic[key]\n",
    "            for v in value:\n",
    "                setValue.discard(v)\n",
    "    #print(\"BLACKLIST:\")\n",
    "    #print(blackList)\n",
    "    for key, value in dic.items():\n",
    "        #print(\"BEFORE:\")\n",
    "        #print(value)\n",
    "        value = value - blackList\n",
    "        dic[key] = value\n",
    "        #print(\"AFTER:\")\n",
    "        #print(value)\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "def getStatsForValues(df, columns): \n",
    "    numerical = {}\n",
    "    categorical = {}\n",
    "    text = {}\n",
    "    for column in columns:\n",
    "        df[column] = pd.to_numeric(df[column], errors='ignore')\n",
    "        columnType = df[column].dtype\n",
    "        if (columnType in numerics):\n",
    "            stats = df[column].describe()\n",
    "            min = stats['min']\n",
    "            max = stats['max']\n",
    "            mean = stats['mean']\n",
    "            median = stats['50%']\n",
    "            std = stats['std']\n",
    "            stats = (min, max, mean, median, std)\n",
    "            numerical[column] = stats\n",
    "        else:\n",
    "            lenValue = df[column].nunique()\n",
    "            lenColumn = df[column].shape[0]\n",
    "            threshold = lenColumn * 0.9\n",
    "            if (lenValue < threshold):\n",
    "                #compute frequencies\n",
    "                freq = df[column].value_counts()\n",
    "                categorical[column] = freq\n",
    "            else:\n",
    "                mean = df[column].map(len).mean()\n",
    "                std = df[column].map(len).std()\n",
    "                median = df[column].map(len).median()\n",
    "                tmpCol = df[column].sort_values()\n",
    "                min = tmpCol.iloc[0]\n",
    "                max = tmpCol.iloc[-1]\n",
    "                stats = (min, max, mean, median, std)\n",
    "                numerical[column] = stats\n",
    "\n",
    "    return numerical, categorical, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ebcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAmbiguousForCategorical(categorical):\n",
    "    colSet = set()\n",
    "    ambiguousValuesForAttr = {}\n",
    "    setPairs = {}\n",
    "    for c1, freq1 in categorical.items():\n",
    "        for c2, freq2 in categorical.items():\n",
    "            if (c1 != c2):\n",
    "                keys1 = freq1.keys()\n",
    "                keys2 = freq2.keys()\n",
    "                ambiguousValues = set(keys1).intersection(set(keys2))\n",
    "                if (len(ambiguousValues) < 1):\n",
    "                    continue\n",
    "                colSet.add(c1)\n",
    "                colSet.add(c2)\n",
    "                if c1 not in ambiguousValuesForAttr:\n",
    "                    ambiguousValuesForAttr[c1] = set(ambiguousValues)\n",
    "                else:\n",
    "                    setValue = ambiguousValuesForAttr[c1]\n",
    "                    setValue = setValue | ambiguousValues\n",
    "                    ambiguousValuesForAttr[c1] = setValue\n",
    "                if c1 not in setPairs:\n",
    "                    attrs = set()\n",
    "                    attrs.add(c2)\n",
    "                    setPairs[c1] = attrs\n",
    "                else:\n",
    "                    attrs = setPairs[c1]\n",
    "                    attrs.add(c2)\n",
    "    return colSet, ambiguousValuesForAttr, setPairs\n",
    "\n",
    "def getIntersection(min1, max1, min2, max2):\n",
    "    ## check type\n",
    "    if (type(min1) == str) or (type(max1) == str) or (type(min2) == str) or (type(max2) == str):\n",
    "        return None\n",
    "    ## check intervals\n",
    "    if (max1 < min1) or (max2 < min2):\n",
    "        print(\"Error min1-max1: {}-{} min2-max2:{}-{}\".format(min1, max1, min2, max2))\n",
    "        return None\n",
    "    # case min1 - max1, min2 - max2 --> no intersection\n",
    "    if (max1 < min2):\n",
    "        return None\n",
    "    # case min2 - max2, min1 - max2 --> no intersection\n",
    "    if (max2 < min1):\n",
    "        return None\n",
    "    # case min1, min2, max1, max2 --> min2-max1\n",
    "    if (min1 <= min2) and (min2 <= max1) and (max1 <= max2):\n",
    "        return (min2, max1)\n",
    "    # case min2, min1, max2, max1 --> min2-max1\n",
    "    if (min2 <= min1) and (min1 <= max2) and (max2 <= max1):\n",
    "        return (min1, max2)\n",
    "    # case min1, min2, max2, max1 --> min2-max2\n",
    "    if (min1 <= min2) and (min2 <= max2) and (max2 <= max1):\n",
    "        return (min2, max2)\n",
    "    # case min2, min1, max1, max2 --> min1-max1\n",
    "    return (min1, max1)\n",
    "\n",
    "\n",
    "def findAmbiguousForNumerical(numerical):\n",
    "    ## naive strategy: intersection of intervals\n",
    "    colSet = set()\n",
    "    ambiguousValuesForAttr = {}\n",
    "    setPairs = {}\n",
    "    for c1, stats1 in numerical.items():\n",
    "        for c2, stats2 in numerical.items():\n",
    "            if (c1 != c2):\n",
    "                interval = getIntersection(stats1[0], stats1[1], stats2[0], stats2[1])\n",
    "                if (interval is None):\n",
    "                    continue\n",
    "                colSet.add(c1)\n",
    "                colSet.add(c2)\n",
    "                if c1 not in ambiguousValuesForAttr:\n",
    "                    listValue = []\n",
    "                    listValue.append(interval)\n",
    "                    ambiguousValuesForAttr[c1] = listValue\n",
    "                else:\n",
    "                    listValue = ambiguousValuesForAttr[c1]\n",
    "                    listValue.append(interval)\n",
    "                if c1 not in setPairs:\n",
    "                    attrs = set()\n",
    "                    attrs.add(c2)\n",
    "                    setPairs[c1] = attrs\n",
    "                else:\n",
    "                    attrs = setPairs[c1]\n",
    "                    attrs.add(c2)\n",
    "    return colSet, ambiguousValuesForAttr, setPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c335e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAmbiguousForLabelAndData(setPairs, setPairsCategorical, setPairsNumerical, ambiguousValuesForAttrCategorical, ambiguousValuesForAttrNumerical):\n",
    "    setPairsLabelData = {}\n",
    "    ambiguousValues = {}\n",
    "    for attributeLabel, ambiguousAttributesLabel in setPairs.items():\n",
    "        setAmb = set()\n",
    "        for attributeCategorical, ambiguousAttributesCategorical in setPairsCategorical.items():\n",
    "            if (attributeLabel == attributeCategorical):\n",
    "                intersection = ambiguousAttributesLabel.intersection(ambiguousAttributesCategorical)\n",
    "                #intersection = list(intersection)\n",
    "                if (len(intersection) > 0):\n",
    "                    for amb in intersection:\n",
    "                        setAmb.add(amb)\n",
    "                    for attr in intersection:\n",
    "                        valuesAmb = ambiguousValuesForAttrCategorical[attr]\n",
    "                        for value in valuesAmb:\n",
    "                            if value in ambiguousValues:\n",
    "                                setAttrs = ambiguousValues[value]\n",
    "                                setAttrs.add(attr)\n",
    "                            else:\n",
    "                                setAttrs = set()\n",
    "                                setAttrs.add(attr)\n",
    "                                ambiguousValues[value] = setAttrs\n",
    "        for attributeNumerical, ambiguousAttributesNumerical in setPairsNumerical.items():\n",
    "            if (attributeLabel == attributeNumerical):\n",
    "                intersection = ambiguousAttributesLabel.intersection(ambiguousAttributesNumerical)\n",
    "                #intersection = list(intersection)\n",
    "                if (len(intersection) > 0):\n",
    "                    for amb in intersection:\n",
    "                        setAmb.add(amb)\n",
    "                    print(intersection)\n",
    "                    for attr in intersection:\n",
    "                        intervals = ambiguousValuesForAttrNumerical[attr]\n",
    "                        for interval in intervals:\n",
    "                            minI = interval[0]\n",
    "                            maxI = interval[1]\n",
    "                            if minI in ambiguousValues:\n",
    "                                setAttrs = ambiguousValues[minI]\n",
    "                                setAttrs.add(attr)\n",
    "                            else:\n",
    "                                setAttrs = set()\n",
    "                                setAttrs.add(attr)\n",
    "                                ambiguousValues[minI] = setAttrs\n",
    "                            if maxI in ambiguousValues:\n",
    "                                setAttrs = ambiguousValues[maxI]\n",
    "                                setAttrs.add(attr)\n",
    "                            else:\n",
    "                                setAttrs = set()\n",
    "                                setAttrs.add(attr)\n",
    "                                ambiguousValues[maxI] = setAttrs\n",
    "        setPairsLabelData[attributeLabel] = setAmb\n",
    "    return setPairsLabelData, ambiguousValues\n",
    "\n",
    "def reversedMap(map):\n",
    "    reversed = {}\n",
    "    for attrName, values in map.items():\n",
    "        for value in values:\n",
    "            #print(attrName, value)\n",
    "            if (value not in reversed):\n",
    "                    attrSet = set()\n",
    "                    attrSet.add(attrName)\n",
    "                    reversed[value] = attrSet\n",
    "            else:\n",
    "                    attrSet = reversed[value]\n",
    "                    attrSet.add(attrName)\n",
    "            #if type(value) is tuple:\n",
    "            #    print(\"TUPLE\")    \n",
    "    return reversedMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValues(line):\n",
    "    s = line.strip()\n",
    "    splitsAttr1 = s.split('attr1: ')\n",
    "    prevVal = splitsAttr1[0]\n",
    "    splitsAttr1 = splitsAttr1[1]\n",
    "    attr2 = splitsAttr1.split('attr2: ')\n",
    "    a1 = attr2[0]\n",
    "    splita2 = attr2[1].split(\"\\t\")\n",
    "    attr1Value = a1.strip()\n",
    "    attr2Value = splita2[0].strip()\n",
    "    labels = splita2[1].strip()\n",
    "    return attr1Value, attr2Value, labels, prevVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConceptNet Func\n",
    "def getConceptNet(word, mode='intersection', limit=10, dropWord=True, useCache=True):\n",
    "    ## modes = ['intersection', 'union']\n",
    "    synonym, _ = getSynonym(word, limit=10, dropWord=True, useCache=True)\n",
    "    relatedTo, _ = getRelatedTo(word, limit=10, dropWord=True, useCache=True)\n",
    "    derivedFrom, _ = getDerivedFrom(word, limit=10, dropWord=True, useCache=True)\n",
    "    isA, _ = getIsA(word, limit=10, dropWord=True, useCache=True)\n",
    "    if mode == 'intersection':\n",
    "        return synonym.intersection(relatedTo, derivedFrom, isA)\n",
    "    else:\n",
    "        return synonym.union(relatedTo, derivedFrom, isA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5777c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASELINES IMPLEMENTATION\n",
    "def baseline_labels(column1, column2):\n",
    "    labels1, _ = getLabel([column1], limit=10, useStemming=True, useLCS=False)\n",
    "    labels2, _ = getLabel([column2], limit=10, useStemming=True, useLCS=False)\n",
    "    commonLabels = labels1[column1].intersection(labels2[column2])\n",
    "    return commonLabels\n",
    "\n",
    "def baseline_conceptnet_function(column1, column2, function):\n",
    "    ## functions = ['synonym', 'relatedTo', 'derivedFrom','isA']\n",
    "    set1 = {}\n",
    "    set2 = {}\n",
    "    if function == 'synonym':\n",
    "        set1, _ = getSynonym(column1, limit=10, dropWord=True, useCache=True)\n",
    "        set2, _ = getSynonym(column2, limit=10, dropWord=True, useCache=True)\n",
    "    if function == 'relatedTo':\n",
    "        set1, _ = getRelatedTo(column1, limit=10, dropWord=True, useCache=True)\n",
    "        set2, _ = getRelatedTo(column2, limit=10, dropWord=True, useCache=True)\n",
    "    if function == 'derivedFrom':\n",
    "        set1, _ = getDerivedFrom(column1, limit=10, dropWord=True, useCache=True)\n",
    "        set2, _ = getDerivedFrom(column2, limit=10, dropWord=True, useCache=True)\n",
    "    if function == 'isA':\n",
    "        set1, _ = getIsA(column1, limit=10, dropWord=True, useCache=True)\n",
    "        set2, _ = getIsA(column2, limit=10, dropWord=True, useCache=True)\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "def baseline_conceptnet(column1, column2, mode):\n",
    "    set1 = getConceptNet(column1, mode, limit=10, dropWord=True, useCache=True)\n",
    "    set2 = getConceptNet(column2, mode, limit=10, dropWord=True, useCache=True)\n",
    "    return set1.intersection(set2)\n",
    "\n",
    "def baseline_wikipedia(column1, column2):\n",
    "    wikipediaColumn1,_ = getAmbiguityFromWikipedia(column1, useStemming=True, results=2, useCache=True)\n",
    "    wikipediaColumn2,_ = getAmbiguityFromWikipedia(column2, useStemming=True, results=2, useCache=True)\n",
    "    return wikipediaColumn1.intersection(wikipediaColumn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Labels Baseline\n",
    "column1 = \"soccer_player\"\n",
    "column2 = \"player\"\n",
    "labels1, _ = getLabel([column1], limit=10, useStemming=True, useLCS=False)\n",
    "labels2, _ = getLabel([column2], limit=10, useStemming=True, useLCS=False)\n",
    "#print(labels1[column1])\n",
    "#print(labels2[column2])\n",
    "commonLabels = labels1[column1].intersection(labels2[column2])\n",
    "print(\"Labels: \", commonLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Synonyms Baseline\n",
    "column1 = \"soccer_player\"\n",
    "column2 = \"player\"\n",
    "synonym1, _ = getSynonym(column1, limit=10, dropWord=True, useCache=True)\n",
    "synonym2, _ = getSynonym(column2, limit=10, dropWord=True, useCache=True)\n",
    "print(synonym1)\n",
    "print(synonym2)\n",
    "commonLabels = synonym1.intersection(synonym2)\n",
    "print(\"Labels: \", commonLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef176d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ConceptNet Baseline\n",
    "column1 = \"soccer_player\"\n",
    "column2 = \"player\"\n",
    "conceptnet1Intersection = getConceptNet(column1, mode='intersection', limit=10, dropWord=True, useCache=True)\n",
    "conceptnet2Intersection = getConceptNet(column2, mode='intersection', limit=10, dropWord=True, useCache=True)\n",
    "print(conceptnet1Intersection)\n",
    "print(conceptnet2Intersection)\n",
    "conceptnet1Union = getConceptNet(column1, mode='union', limit=10, dropWord=True, useCache=True)\n",
    "conceptnet2Union = getConceptNet(column2, mode='union', limit=10, dropWord=True, useCache=True)\n",
    "print(conceptnet1Union)\n",
    "print(conceptnet2Union)\n",
    "commonLabelsIntersection = conceptnet1Intersection.intersection(conceptnet2Intersection)\n",
    "commonLabelsUnion = conceptnet1Union.intersection(conceptnet2Union)\n",
    "print(commonLabelsIntersection)\n",
    "print(commonLabelsUnion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eae3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Wikipedia Baseline\n",
    "column1 = \"soccer_player\"\n",
    "column2 = \"player\"\n",
    "wikipediaColumn1,_ = getAmbiguityFromWikipedia(column1, useStemming=True, results=2, useCache=True)\n",
    "wikipediaColumn2,_ = getAmbiguityFromWikipedia(column2, useStemming=True, results=2, useCache=True)\n",
    "#print(wikipediaColumn1)\n",
    "#print(wikipediaColumn2)\n",
    "commonLabels = wikipediaColumn1.intersection(wikipediaColumn2)\n",
    "print(\"Labels: \", commonLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LCS Baseline\n",
    "column1 = \"soccer_player\"\n",
    "column2 = \"player\"\n",
    "lcs = getLCS(column1, column2)\n",
    "print(\"Labels:\", lcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e585481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractValues(x):\n",
    "    s = x.replace('[', '').replace(']','')\n",
    "    if len(s) == 0:\n",
    "        return set()\n",
    "    else:\n",
    "        splits = s.split(sep=',')\n",
    "        values = []\n",
    "        for split in splits:\n",
    "            tmp = split.strip().replace(\"'\", \"\")\n",
    "            values.append(tmp)\n",
    "    return set(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeba196",
   "metadata": {},
   "source": [
    "# Run all baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD TEST DATA\n",
    "file = './../data/test-task1-manual.tsv'\n",
    "fileTask = open(file, 'r')\n",
    "lines = fileTask.readlines()\n",
    "examples = []\n",
    "count = 0\n",
    "for line in lines:\n",
    "    column1, column2, labelsActual, _ = getValues(line)\n",
    "    labelsPrediction = baseline_labels(column1, column2)\n",
    "    synonym = baseline_conceptnet_function(column1, column2, 'synonym')\n",
    "    relatedTo = baseline_conceptnet_function(column1, column2, 'relatedTo')\n",
    "    derivedFrom = baseline_conceptnet_function(column1, column2, 'derivedFrom')\n",
    "    isA = baseline_conceptnet_function(column1, column2, 'isA')\n",
    "    conceptNetUnion = baseline_conceptnet(column1, column2, 'union')\n",
    "    conceptNetIntersection = baseline_conceptnet(column1, column2, 'intersection')\n",
    "    wikipedia = baseline_wikipedia(column1, column2)\n",
    "    lcs = getLCS(column1, column2)\n",
    "    t = (column1, column2, labelsActual, labelsPrediction, synonym, relatedTo, derivedFrom, isA, conceptNetUnion, conceptNetIntersection, wikipedia, lcs)\n",
    "    examples.append(t)\n",
    "    count += 1\n",
    "    print(\"Examples processed: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d621bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(targets, predictions):\n",
    "    hits = 0\n",
    "    for tgt, pred in zip(targets, predictions):\n",
    "        if len(tgt) == 0:\n",
    "            if len(pred) == 0:\n",
    "                hits += 1\n",
    "        else:\n",
    "            valuesInCommon = tgt.intersection(pred)\n",
    "            if len(valuesInCommon) > 0:\n",
    "                hits += 1\n",
    "    total = len(targets)\n",
    "    accuracy = hits/total\n",
    "    return accuracy\n",
    "\n",
    "def countStatsLabelsStats(targets, predictions):\n",
    "    count_pairs = 0\n",
    "    hits = 0\n",
    "    for tgt, pred in zip(targets, predictions):\n",
    "        if len(tgt) > 0:\n",
    "            count_pairs += 1\n",
    "        valuesInCommon = tgt.intersection(pred)\n",
    "        if len(valuesInCommon) > 0:\n",
    "            hits += 1\n",
    "    return count_pairs, hits\n",
    "            \n",
    "def countStats(targets, predictions):\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    wrongLabel = 0\n",
    "    for tgt, pred in zip(targets, predictions):\n",
    "        if len(tgt) == 0:\n",
    "            if len(pred) == 0:\n",
    "                tn += 1\n",
    "                continue\n",
    "            else:\n",
    "                fp += 1\n",
    "                continue\n",
    "        if len(pred) == 0:\n",
    "            if len(tgt) > 0:\n",
    "                fn += 1\n",
    "                continue\n",
    "        valuesInCommon = tgt.intersection(pred)\n",
    "        if len(valuesInCommon) > 0:\n",
    "            tp += 1\n",
    "        else:\n",
    "            wrongLabel += 1\n",
    "    return tn, fp, tp, fn, wrongLabel\n",
    "\n",
    "def precision(targets, predictions):\n",
    "    tn, fp, tp, fn, wrongLabel = countStats(targets, predictions)\n",
    "    if (tp + fp) == 0:\n",
    "        return 0, wrongLabel, 0\n",
    "    precision = tp /(tp + fp)\n",
    "    precisionBinary = (tp + wrongLabel)/(tp + wrongLabel + fp)\n",
    "    return precision, wrongLabel, precisionBinary\n",
    "        \n",
    "def recall(targets, predictions):\n",
    "    tn, fp, tp, fn, wrongLabel = countStats(targets, predictions)\n",
    "    if (tp + fn) == 0:\n",
    "        return 0, wrongLabel, 0\n",
    "    #recall = tp /(tp + fn)\n",
    "    recall = tp /(tp + fn + wrongLabel)\n",
    "    recallBinary = (tp + wrongLabel) / (tp + fn + wrongLabel)\n",
    "    return recall, wrongLabel, recallBinary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "actuals = []\n",
    "synonyms = []\n",
    "relatedsTo = []\n",
    "derivedsFrom = []\n",
    "isAs = []\n",
    "conceptNetUnions = []\n",
    "conceptNetIntersections = []\n",
    "wikipedias = []\n",
    "lcss = []\n",
    "columns1 = []\n",
    "columns2 = []\n",
    "for example in examples:\n",
    "    columns1.append(example[0])\n",
    "    columns2.append(example[1])\n",
    "    actual = example[2]\n",
    "    labelsPrediction = example[3]\n",
    "    synonym = example[4]\n",
    "    relatedTo = example[5]\n",
    "    derivedFrom = example[6]\n",
    "    isA = example[7]\n",
    "    conceptNetUnion = example[8]\n",
    "    conceptNetIntersection = example[9]\n",
    "    wikipedia = example[10]\n",
    "    lcs = example[11]\n",
    "    actuals.append(extractValues(actual))\n",
    "    labels.append(labelsPrediction)\n",
    "    synonyms.append(synonym)\n",
    "    relatedsTo.append(relatedTo)\n",
    "    derivedsFrom.append(derivedFrom)\n",
    "    isAs.append(isA)\n",
    "    conceptNetUnions.append(conceptNetUnion)\n",
    "    conceptNetIntersections.append(conceptNetIntersection)\n",
    "    wikipedias.append(wikipedia)\n",
    "    lcss.append(lcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline\",\"Accuracy\", \"Precision\", 'Recall','Precision Binary', 'Recall Binary','Hits','Ambiguous Pairs', sep=\"\\t\")\n",
    "\n",
    "accuracyLabels = accuracy(actuals, labels)\n",
    "precisionLabels, _, precisionBinary_labels = precision(actuals, labels)\n",
    "recallLabels, _ ,recallBinary_labels = recall(actuals, labels)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, labels)\n",
    "print(\"labels\",accuracyLabels, precisionLabels, recallLabels, precisionBinary_labels, recallBinary_labels,count_pairs, hits, sep=\"\\t\")\n",
    "\n",
    "accuracysynonyms = accuracy(actuals, synonyms)\n",
    "precisionsynonyms, _ , precisionBinary_synonyms= precision(actuals, synonyms)\n",
    "recallsynonyms, _ ,recallBinary_synonyms  = recall(actuals, synonyms)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, synonyms)\n",
    "print(\"synonyms\",accuracysynonyms, precisionsynonyms, recallsynonyms,precisionBinary_synonyms,recallBinary_synonyms,count_pairs, hits,  sep=\"\\t\")\n",
    "\n",
    "accuracyrelatedTo = accuracy(actuals, relatedsTo)\n",
    "precisionrelatedTo, _ , precisionBinary_relatedsTo = precision(actuals, relatedsTo)\n",
    "recallrelatedTo, _ ,recallBinary_relatedsTo  = recall(actuals, relatedsTo)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, relatedsTo)\n",
    "print(\"relatedsTo\",accuracyrelatedTo, precisionrelatedTo, recallrelatedTo,precisionBinary_relatedsTo,recallBinary_relatedsTo ,count_pairs, hits, sep=\"\\t\")\n",
    "\n",
    "accuracyderivedsFrom = accuracy(actuals, derivedsFrom)\n",
    "precisionderivedsFrom, _ , precisionBinary_derivedsFrom = precision(actuals, derivedsFrom)\n",
    "recallderivedsFrom, _ ,recallBinary_derivedsFrom  = recall(actuals, derivedsFrom)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, derivedsFrom)\n",
    "print(\"derivedsFrom\",accuracyderivedsFrom, precisionderivedsFrom, recallderivedsFrom,precisionBinary_derivedsFrom,recallBinary_derivedsFrom, count_pairs, hits, sep=\"\\t\")\n",
    "\n",
    "accuracyisAs = accuracy(actuals, isAs)\n",
    "precisionisAs, _ , precisionBinary_derivedsFrom = precision(actuals, isAs)\n",
    "recallisAs, _ ,recallBinary_isAs  = recall(actuals, isAs)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, isAs)\n",
    "print(\"isAs\",accuracyisAs, precisionisAs, recallisAs,precisionBinary_derivedsFrom,recallBinary_isAs,count_pairs, hits,  sep=\"\\t\")\n",
    "\n",
    "accuracyconceptNetUnions = accuracy(actuals, conceptNetUnions)\n",
    "precisionconceptNetUnions, _ , precisionBinary_conceptNetUnions = precision(actuals, conceptNetUnions)\n",
    "recallconceptNetUnions, _ ,recallBinary_conceptNetUnions  = recall(actuals, conceptNetUnions)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, conceptNetUnions)\n",
    "print(\"conceptNetUnions\",accuracyconceptNetUnions, precisionconceptNetUnions, recallconceptNetUnions,precisionBinary_conceptNetUnions,recallBinary_conceptNetUnions, count_pairs, hits, sep=\"\\t\")\n",
    "\n",
    "accuracyconceptNetIntersections = accuracy(actuals, conceptNetIntersections)\n",
    "precisionconceptNetIntersections, _ , precisionBinary_conceptNetIntersections = precision(actuals, conceptNetIntersections)\n",
    "recallconceptNetIntersections, _ ,recallBinary_conceptNetIntersections  = recall(actuals, conceptNetIntersections)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, conceptNetIntersections)\n",
    "print(\"conceptNetIntersections\",accuracyconceptNetIntersections, precisionconceptNetIntersections, recallconceptNetIntersections,precisionBinary_conceptNetIntersections, recallBinary_conceptNetIntersections, count_pairs, hits, sep=\"\\t\")\n",
    "\n",
    "accuracywikipedias = accuracy(actuals, wikipedias)\n",
    "precisionwikipedias, _ , precisionBinary_wikipedias = precision(actuals, wikipedias)\n",
    "recallwikipedias, _ ,recallBinary_wikipedias  = recall(actuals, wikipedias)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, wikipedias)\n",
    "print(\"wikipedias\",accuracywikipedias, precisionwikipedias, recallwikipedias,precisionBinary_wikipedias, recallBinary_wikipedias,count_pairs, hits, sep=\"\\t\")\n",
    "\n",
    "accuracylcss = accuracy(actuals, lcss)\n",
    "precisionlcss, _ , precisionBinary_lcss = precision(actuals, lcss)\n",
    "recalllcss, _ ,recallBinary_lcss  = recall(actuals, lcss)\n",
    "count_pairs, hits = countStatsLabelsStats(actuals, lcss)\n",
    "print(\"lcss\",accuracylcss, precisionlcss, recalllcss,precisionBinary_lcss,recallBinary_lcss,count_pairs, hits,  sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c1, c2, actual, label, lcs in zip(columns1, columns2, actuals, labels, lcss):\n",
    "    print(c1, c2, actual, label, lcs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
