{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"SIGMOD-AmbTotto.ipynb","provenance":[{"file_id":"15ZKGvhjHNQBL7QPnA24rOCcih6JF69Fc","timestamp":1631996826845}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOrpFmq4tvXH3Csfkrcp6iM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H2ORHhuAfvzr"},"source":["## Code for automatically click connect (run on console cmd+alt+i)\n","```\n","function KeepClicking(){\n","console.log(\"Clicking\");\n","document.querySelector(\"colab-connect-button\").click()\n","}\n","setInterval(KeepClicking,60000)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"bhnCpGHpdCW-"},"source":["print(\"Installing dependencies...\")\n","model_dir_name = \"models\" # TOTTO finetuning model\n","#model_dir_name = \"noTottoModels\"\n","%tensorflow_version 2.x\n","!pip install -q t5\n","\n","# copy t5 model\n","# gsutil cp -r \"gs://t5-data/pretrained_models/3B/\" \"gs://totto-explain-claim/models/\"\n","\n","import functools\n","import os\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","\n","import t5\n","import t5.models\n","\n","BASE_DIR = \"gs://pythia_totto\" #@param { type: \"string\" }\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BASE_DIR.\")\n","DATA_DIR = os.path.join(BASE_DIR, \"data\")\n","MODELS_DIR = os.path.join(BASE_DIR, model_dir_name)\n","print(\"MODELS_DIR:\", MODELS_DIR)\n","ON_CLOUD = True\n","\n","\n","if ON_CLOUD:\n","  print(\"Setting up GCS access...\")\n","  import tensorflow_gcs_config\n","  from google.colab import auth\n","  # Set credentials for GCS reading/writing from Colab and TPU.\n","  TPU_TOPOLOGY = \"v2-8\"\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    TPU_ADDRESS = tpu.get_master()\n","    print('Running on TPU:', TPU_ADDRESS)\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","  auth.authenticate_user()\n","  tf.enable_eager_execution()\n","  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n","\n","tf.disable_v2_behavior()\n","\n","# Improve logging.\n","from contextlib import contextmanager\n","import logging as py_logging\n","\n","if ON_CLOUD:\n","  tf.get_logger().propagate = False\n","  py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YS_i8d26GSuZ"},"source":["# Import PyDrive and associated libraries.\n","# This only needs to be done once in a notebook.\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once in a notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hx-dFzVQejxY"},"source":["print(\"DATA_DIR:\", DATA_DIR)\n","print(\"MODELS_DIR:\", MODELS_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lHPLFtdpde_c"},"source":["!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUxuHXMTKTMV"},"source":["print(t5.data.MixtureRegistry.names())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uKLkCHkEJc9r"},"source":["def toList(lst):\n","    return list(map(lambda el:[el], lst))\n","\n","def toLower(lst):\n","    return list(map(lambda x:x.lower(), lst))\n","\n","def toTokens(lst):\n","    return list(map(str.split, lst))\n","\n","#l = ['Uno Due Tre', 'Uno Due', 'UNO']\n","#print(toList(l))\n","#print(toLower(l))\n","#print(toTokens(toLower(l)))\n","#print(toList(toTokens(toLower(l))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiYFBczQs3Ro"},"source":["def compare(str1, str2): \n","  return str1.lower().strip() == str2.lower().strip()\n","\n","def evaluate(predicted, actual):\n","  tp = 0\n","  for i in range(0, len(predicted)):\n","    pred = predicted[i]\n","    act = actual[i]\n","    if (compare(pred, act)):\n","      tp += 1\n","  return (tp / len(predicted))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDulWcODeQmh"},"source":["from datasets import list_datasets, load_dataset, list_metrics, load_metric\n","totto_dataset = load_dataset('totto')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQN-KTi1mKFO"},"source":["class Header:\n","  def __init__(self, name):\n","    self.name = name\n","\n","  def __str__(self):\n","    return \"<col_header> \" + self.name + \" </col_header>\"\n","\n","\n","class Cell:\n","  def __init__(self, value, header):\n","    self.header = header\n","    self.value = value\n","\n","  def __str__(self):\n","    if (self.header is None):\n","      return \"<cell> \" + self.value +\" </cell>\"\n","    else:\n","      return \"<cell> \" + self.value + \" \" + str(self.header) + \" </cell>\"\n","\n","\n","class Table:\n","  def __init__(self):\n","    self.rows = []\n","  \n","  def initTable(self, data):\n","    headersList = []\n","    currentHeader = []\n","    rowHeader = -1\n","    rowIndex = 0\n","    for dataRow in data:\n","      currentRow = []\n","      #print(\"****ROW: \", dataRow)\n","      #print(\"****ROW_INDEX:\", rowIndex)\n","      #print(\"****CURRENT HEADER:\", len(currentHeader))\n","      isHeaderRow = self.__containsHeader(dataRow)\n","      if (isHeaderRow):\n","        #print(\"****FOUND HEADER\")\n","        header = self.__readHeader(dataRow)\n","        rowHeader = rowIndex\n","        currentHeader = header\n","        headersList.append(currentHeader)\n","        currentRow = self.__readRow(dataRow)\n","      else:\n","        currentRow = self.__readRow(dataRow, currentHeader, headersList)\n","      rowIndex += 1\n","      self.rows.append(currentRow)\n","\n","  def __readHeader(self, row):\n","    header = []\n","    for cell in row:\n","      rowSpan = cell['column_span']\n","      value = cell['value']\n","      for i in range(rowSpan):\n","        header.append(Header(value))\n","    return header\n","\n","  def __readRow(self, row, currentHeader = None, headersList = None):\n","    tableRow = []\n","    colIndex = 0\n","    matchedHeader = self.__match(row, currentHeader, headersList)\n","    #print(\"****MATCHED_HEADER:\", matchedHeader)\n","    for cell in row:\n","      rowSpan = cell['column_span']\n","      value = cell['value']\n","      for i in range(rowSpan):\n","        if ((matchedHeader is None) or (len(matchedHeader) == 0)):\n","          tableRow.append(Cell(value, None))\n","        else:\n","          tableRow.append(Cell(value, matchedHeader[colIndex]))\n","      colIndex += 1\n","    return tableRow\n","\n","  def __match(self, row, currentHeader, headersList):\n","    if ((currentHeader is None) and (headersList is None)):\n","      return None\n","    if ((currentHeader is not None) and (len(row) == len(currentHeader))):\n","      return currentHeader\n","    if ((headersList is not None) and (len(headersList) > 0)):\n","      for header in headersList:\n","        if (len(header) == len(row)):\n","          return header\n","    return None\n","\n","  def __containsHeader(self, cellData):\n","    for cell in cellData:\n","      if (bool(cell['is_header'])):\n","        return True\n","    return False\n","\n","  def cellsToText(self, cellsList):\n","    text = \"\"\n","    for cell in cellsList:\n","      cellData = self.rows[cell[0]][cell[1]]\n","      text += str(cellData) + \" \"\n","    return \"<table> \" + text + \"</table>\"\n","\n","  def __str__(self):\n","    textData = \"\"\n","    for row in self.rows:\n","      textData += \" \".join([str(cell) for cell in row])+\"\\n\"\n","    return textData\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZuq3krQU5YX"},"source":["class TottoTable:\n","\n","  def __init__(self):\n","    self.headers = []\n","    self.rows = []\n","\n","  def initTable(self, data):\n","    for rowData in data:\n","      for cellData in rowData:\n","        if (bool(cellData['is_header'])):\n","           rowSpan = cellData['column_span']\n","           for i in range(rowSpan):\n","             self.headers.append(cellData['value'])\n","        else:\n","          rowSpan = cellData['column_span']\n","          row = []\n","          for i in range(rowSpan):\n","            self.headers.append(cellData['value'])\n","            row.append(cellData['value'])\n","          self.rows.append(row)\n","\n","\n","  def setHeaders(self, headersList):\n","    for headerCell in headersList:\n","      rowSpan = headerCell['column_span']\n","      for i in range(1, rowSpan):\n","        self.headers.append(headerCell['value'])\n","\n","  def addRow(self, rowList):\n","    row = []\n","    for rowCell in rowList:\n","      row.append(rowCell['value'])\n","    self.rows.append(row)\n","  \n","  def getHeader(self, pos):\n","    return self.headers[pos]\n","\n","  def getCellValue(self, row, column):\n","    return self.rows[row][column]\n","\n","  def __str__(self):\n","    headerString = \"\\t\".join(self.headers) + \"\\n\"\n","    dataString = \"\"\n","    for row in self.rows:\n","      dataString += \"\\t\".join(row) + \"\\n\"\n","    return headerString + dataString\n","\n","  def cellsToText(self, cellsList):\n","    text = \"\"\n","    for cell in cellsList:\n","      headerValue = \"<col_header> \"+ self.headers[cell[1]] + \" </col_header>\"\n","      cellValue =\"<cell> \"+ self.getCellValue(cell[0]-1, cell[1])+ \" \" + headerValue + \" </cell>\"\n","      text += cellValue+ \" \"\n","    return \"<table> \" + text + \"</table>\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vo5GQZc-eVjS"},"source":["def dataset_to_tsv(dataset, out_fname):\n","  count = 0\n","  with tf.io.gfile.GFile(out_fname, \"w\") as outfile:\n","    for example in dataset:\n","      table_cells_text = \" \".join((map(lambda cell: str(cell), example['table'])))\n","      highlighted_cells_text = \" \".join((map(lambda cell: str(example['table'][cell[0]][cell[1]]), example['highlighted_cells'])))\n","      text_to_append = [\"<page_title>\", example['table_page_title'], \"/<page_title>\", \"<section_title>\", example['table_section_title'], \"</section_title>\", \"<table>\", table_cells_text, \"</table>\", \"<highlighted_cells>\", highlighted_cells_text, \"</highlighted_cells>\"]\n","      x = \" \".join(text_to_append)\n","      y = example['sentence_annotations']['final_sentence'][0]\n","      outfile.write(\"%s\\t%s\\n\" % (x, y))\n","      count+=1\n","      tf.logging.log_every_n(tf.logging.INFO, \"Wrote %d examples to %s.\" % (count, out_fname), 1000)\n","  return count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2hFZCXkC3Zv"},"source":["def dataset_to_tsv_2(dataset, out_fname):\n","  count = 0\n","  with tf.io.gfile.GFile(out_fname, \"w\") as outfile:\n","    for example in dataset:\n","      #print(\"Example N:\", count)\n","      table = example['table']\n","      t = Table();\n","      t.initTable(table)\n","      highlighted_cells = example['highlighted_cells']\n","      highlightet_cells_text = t.cellsToText(highlighted_cells)\n","      text_to_append = [\"<page_title>\", example['table_page_title'], \"</page_title>\", \"<section_title>\", example['table_section_title'], \"</section_title>\", highlightet_cells_text]\n","      x = \" \".join(text_to_append)\n","      y = example['sentence_annotations']['final_sentence'][0]\n","      #print(x)\n","      #print(y)\n","      outfile.write(\"%s\\t%s\\n\" % (x, y))\n","      count+=1\n","      tf.logging.log_every_n(tf.logging.INFO, \"Wrote %d examples to %s.\" % (count, out_fname), 1000)\n","  return count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3-NSK1efINq"},"source":["## DATA FOR TOTTO FINE TUNING - Train with 25000 Steps\n","import json\n","counts_path = os.path.join(DATA_DIR, \"counts.json\")\n","tsv_path = {\n","    \"train\": os.path.join(DATA_DIR, \"train.tsv\"),\n","    \"validation\": os.path.join(DATA_DIR, \"validation.tsv\"),\n","    \"test\": os.path.join(DATA_DIR, \"test.tsv\")\n","}\n","\n","if tf.io.gfile.exists(counts_path):\n","  # Used cached data and counts.\n","  tf.logging.info(\"Loading NQ from cache.\")\n","  num_nq_examples = json.load(tf.io.gfile.GFile(counts_path))\n","else:\n","  # Create TSVs and get counts.\n","  tf.logging.info(\"Generating NQ TSVs.\")\n","  num_nq_examples = {}\n","  for split in [\"train\", \"validation\", \"test\"]:\n","    num_nq_examples[split] = dataset_to_tsv_2(totto_dataset[split], tsv_path[split])\n","  json.dump(num_nq_examples, tf.io.gfile.GFile(counts_path, \"w\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jVpd-e-sY2fg"},"source":["## DATA FOR AMBTOTTO FINE TUNING - To Execute after the first training till 30000 steps\n","import json\n","print(DATA_DIR)\n","#counts_path = os.path.join(DATA_DIR, \"counts.json\")\n","tsv_path = {\n","    \"train\": os.path.join(DATA_DIR, \"totto_train.tsv\"),\n","    \"validation\": os.path.join(DATA_DIR, \"totto_ambiguities.tsv\"),\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HetQjFGVfQMd"},"source":["def to_dataset_ts(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(tsv_path[split])\n","  # Split each \"<question>\\t<answer>\" example into (question, answer) tuple.\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  # Map each tuple to a {\"totto\": ... \"explain_table\": ...} dict.\n","  ds = ds.map(lambda *ex: dict(zip([\"totto\", \"explain_table\"], ex)))\n","  return ds\n","\n","print(\"A few raw validation examples...\")\n","for ex in tfds.as_numpy(to_dataset_ts(\"train\").take(5)):\n","  print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ObuHgYr5fUvP"},"source":["def text_preprocessor(ds):\n","  def normalize_text(text):\n","    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n","    text = tf.strings.lower(text)\n","    #text = tf.strings.regex_replace(text,\"'(.*)'\", r\"\\1\")\n","    return text\n","\n","  def to_inputs_and_targets(ex):\n","    \"\"\"Map {\"question\": ..., \"answer\": ...}->{\"inputs\": ..., \"targets\": ...}.\"\"\"\n","    return {\n","        \"inputs\":\n","             tf.strings.join(\n","                 [\"totto table: \", normalize_text(ex[\"totto\"])]),\n","        \"targets\": normalize_text(ex[\"explain_table\"])\n","    }\n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8XNUfxsfXND"},"source":["t5.data.TaskRegistry.remove(\"totto_context\")\n","t5.data.TaskRegistry.add(\n","    \"totto_context\",\n","    # Specify the task type.\n","    t5.data.Task,\n","    # Supply a function which returns a tf.data.Dataset.\n","    dataset_fn=to_dataset_ts,\n","    splits=[\"train\", \"validation\"],\n","    # Supply a function which preprocesses text from the tf.data.Dataset.\n","    text_preprocessor=[text_preprocessor],\n","    # Lowercase targets before computing metrics.\n","    postprocess_fn=t5.data.postprocessors.lower_text, \n","    # We'll use accuracy as our evaluation metric.\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    # Not required, but helps for mixing and auto-caching.\n","    #num_input_examples=num_nq_examples\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jjC8t6zjhPI3"},"source":["nq_task = t5.data.TaskRegistry.get(\"totto_context\")\n","#ds = nq_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 128, \"targets\": 32})\n","ds = nq_task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","print(\"A few preprocessed validation examples...\")\n","for ex in tfds.as_numpy(ds.take(5)):\n","  print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"25XT__7dhRdl"},"source":["t5.data.MixtureRegistry.remove(\"totto_all\")\n","t5.data.MixtureRegistry.remove(\"all_mix\")\n","t5.data.MixtureRegistry.add(\n","    \"all_mix\",\n","    [\"totto_context\"],\n","     default_rate=1.0\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7GCI462hTRQ"},"source":["MODEL_SIZE = \"3B\" #@param[\"small\", \"base\", \"large\", \"3B\", \"11B\"]\n","# Public GCS path for T5 pre-trained model checkpoints\n","BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n","PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n","MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)\n","\n","if ON_CLOUD and MODEL_SIZE == \"3B\":\n","  tf.logging.warning(\n","      \"The `3B` model is too large to use with the 5GB GCS free tier. \"\n","      \"Make sure you have at least 25GB on GCS before continuing.\"\n","  )\n","elif ON_CLOUD and MODEL_SIZE == \"11B\":\n","  raise ValueError(\n","      \"The `11B` parameter is too large to fine-tune on the `v2-8` TPU \"\n","      \"provided by Colab. Please comment out this Error if you're running \"\n","      \"on a larger TPU.\"\n","  )\n","\n","# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n","# Limit number of checkpoints to fit within 5GB (if possible).\n","model_parallelism, train_batch_size, keep_checkpoint_max = {\n","    \"small\": (1, 256, 16),\n","    \"base\": (2, 128, 8),\n","    \"large\": (8, 64, 4),\n","    \"3B\": (8, 16, 1),\n","    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n","\n","tf.io.gfile.makedirs(MODEL_DIR)\n","# The models from our paper are based on the Mesh Tensorflow Transformer.\n","\n","model = t5.models.MtfModel(\n","    model_dir=MODEL_DIR,\n","    tpu=TPU_ADDRESS,\n","    tpu_topology=TPU_TOPOLOGY,\n","    model_parallelism=model_parallelism,\n","    batch_size=train_batch_size,\n","    sequence_length={\"inputs\": 128, \"targets\": 32},\n","    learning_rate_schedule=0.003,\n","    save_checkpoints_steps=100,\n","    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n","    iterations_per_loop=100,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgNZjCsahgPo"},"source":["if ON_CLOUD:\n","  import datetime\n","  %reload_ext tensorboard\n","  !rm -rf ./logs/\n","  log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","\n","#%tensorboard --logdir=\"$MODEL_DIR\" --port=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m3qr8Z9ShiO6"},"source":["FINETUNE_STEPS =  30000 #@param {type: \"integer\"}\n","model._save_checkpoints_steps=1000\n","model._keep_checkpoint_max = 50\n","\n","model.finetune(\n","    mixture_or_task_name=\"all_mix\",\n","    pretrained_model_dir=PRETRAINED_DIR,\n","    #pretrained_model_dir=MODEL_DIR,\n","    finetune_steps=FINETUNE_STEPS,\n","    #pretrained_checkpoint_step = 12000\n",")\n","\n","MSG = str(FINETUNE_STEPS) + \" done\"\n","push = pb.push_note(\"TOTTO\", MSG)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ueta7iJUXCrp"},"source":["tf.train.latest_checkpoint(\n","    model._model_dir, latest_filename=None\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pR2dS0MjV9T8"},"source":["print(\"Model Batch Size: \", model.batch_size)\n","print(\"Train Batch Size: \", train_batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"szNqpGhAht8S"},"source":["# Use a larger batch size for evaluation, which requires less memory.\n","#model.batch_size = train_batch_size * 4\n","model.batch_size = train_batch_size * 1\n","#model.eval(\n","#    mixture_or_task_name=\"all_mix\",\n","#    checkpoint_steps=\"all\"\n","#)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tnfd5TMgKQ8I"},"source":["!pip install fsspec\n","!pip install gcsfs\n","import pandas as pd\n","from tensorflow.python.lib.io import file_io\n","with file_io.FileIO('gs://pythia_totto/data/totto_ambiguities.tsv', 'r') as f:\n","  df = pd.read_csv(f, sep='\\t', header=None)\n","print(df.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppmUL4CFf-Mb"},"source":["x_val = df[0].values\n","y_val = df[1].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2fttTaCfgVHr"},"source":["now = time.time()\n","# Write out the supplied questions to text files.\n","predict_inputs_path = os.path.join(MODEL_DIR, \"predict_inputs_%d.txt\" % now)\n","predict_outputs_path = os.path.join(MODEL_DIR, \"predict_outputs_%d.txt\" % now)\n","print(predict_inputs_path)\n","print(predict_outputs_path)\n","# Manually apply preprocessing by prepending \"triviaqa question:\".\n","with tf.io.gfile.GFile(predict_inputs_path, \"w\") as f:\n","  for q in x_val:\n","    f.write(\"totto table: %s\\n\" % q.lower())\n","\n","# Ignore any logging so that we only see the model's answers to the questions.\n","with tf_verbosity_level('ERROR'):\n","  model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n","  model.predict(\n","      input_file=predict_inputs_path,\n","      output_file=predict_outputs_path,\n","      # Select the most probable output token at each step.\n","      temperature=0,\n","  )\n","\n","# The output filename will have the checkpoint appended so we glob to get \n","# the latest.\n","prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n","print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])\n","predicted = []\n","with tf.io.gfile.GFile(prediction_files[-1]) as f:\n","  for x, y, p in zip(x_val, y_val, f):\n","    if x:\n","      predicted.append(p)\n","      #print(\"Input     : \" + x)\n","      #print(\"Prediction: \" + p)\n","      #print(\"Actual    : \" + y)\n","      #print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGgmDg0xj_RQ"},"source":["print(len(predicted))\n","print(len(y_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kIl2fT1Rbjs"},"source":["for pred, actual in zip(predicted, y_val):\n","  print(pred, actual)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpYmVQGVoQgC"},"source":["def toList(lst):\n","    return list(map(lambda el:[el], lst))\n","\n","def toLower(lst):\n","    return list(map(lambda x:x.lower(), lst))\n","\n","def toTokens(lst):\n","    return list(map(str.split, lst))\n","\n","l = ['Uno Due Tre', 'Uno Due', 'UNO']\n","print(toList(l))\n","print(toLower(l))\n","print(toTokens(toLower(l)))\n","print(toList(toTokens(toLower(l))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlw01rnwmVSo"},"source":["from datasets import load_metric\n","sacrebleu_metric = load_metric('sacrebleu')\n","#print(sacrebleu_metric)\n","rouge_metric = load_metric('rouge')\n","#print(rouge_metric)\n","bleu_metric = load_metric('bleu')\n","results_sacrebleu = sacrebleu_metric.compute(predictions=predicted, references=toList(y_val), lowercase=True)\n","#print(results_sacrebleu)\n","results_rouge = rouge_metric.compute(predictions=toLower(predicted), references=toLower(y_val))\n","#print(results_rouge)\n","#results_bleu = bleu_metric.compute(predictions=toTokens(toLower(predicted)), references=toList((toTokens(toLower(y_val)))))\n","#print(results_bleu)\n","\n","print(\"BLEU score: \", results_sacrebleu['score'])\n","print(\"ROUGE score: \", results_rouge['rouge1'].low.fmeasure)"],"execution_count":null,"outputs":[]}]}